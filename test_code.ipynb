{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Code: Framework Demonstration\n",
    "\n",
    "This notebook demonstrates how to use the new M5 Benchmarking Framework by recreating the workflow from the original `load_data.ipynb` notebook using the modular classes and functions.\n",
    "\n",
    "## Overview\n",
    "We'll walk through:\n",
    "1. **Data Loading** using `DataLoader` class\n",
    "2. **Feature Engineering** using `FeatureEngineer` class\n",
    "3. **Model Training** using `ModelTrainer` class with Optuna optimization\n",
    "4. **Model Storage** using `ModelRegistry` class\n",
    "5. **Evaluation** using `ModelEvaluator` class\n",
    "6. **Visualization** using `VisualizationGenerator` class\n",
    "7. **Loading and Using Previously Saved Models** (Production scenarios)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Setup\n",
    "\n",
    "First, let's import all the necessary libraries and initialize our framework components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div id=\"vOfk2s\"></div>\n",
       "            <script type=\"text/javascript\" data-lets-plot-script=\"library\">\n",
       "                if(!window.letsPlotCallQueue) {\n",
       "                    window.letsPlotCallQueue = [];\n",
       "                }; \n",
       "                window.letsPlotCall = function(f) {\n",
       "                    window.letsPlotCallQueue.push(f);\n",
       "                };\n",
       "                (function() {\n",
       "                    var script = document.createElement(\"script\");\n",
       "                    script.type = \"text/javascript\";\n",
       "                    script.src = \"https://cdn.jsdelivr.net/gh/JetBrains/lets-plot@v4.6.2/js-package/distr/lets-plot.min.js\";\n",
       "                    script.onload = function() {\n",
       "                        window.letsPlotCall = function(f) {f();};\n",
       "                        window.letsPlotCallQueue.forEach(function(f) {f();});\n",
       "                        window.letsPlotCallQueue = [];\n",
       "                        \n",
       "                    };\n",
       "                    script.onerror = function(event) {\n",
       "                        window.letsPlotCall = function(f) {};    // noop\n",
       "                        window.letsPlotCallQueue = [];\n",
       "                        var div = document.createElement(\"div\");\n",
       "                        div.style.color = 'darkred';\n",
       "                        div.textContent = 'Error loading Lets-Plot JS';\n",
       "                        document.getElementById(\"vOfk2s\").appendChild(div);\n",
       "                    };\n",
       "                    var e = document.getElementById(\"vOfk2s\");\n",
       "                    e.appendChild(script);\n",
       "                })()\n",
       "            </script>\n",
       "            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Framework imported successfully!\n",
      "Polars version: 1.31.0\n",
      "XGBoost version: 3.0.2\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "# ML libraries (same as original notebook)\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# Visualization (same as original notebook)\n",
    "from lets_plot import *\n",
    "LetsPlot.setup_html()\n",
    "\n",
    "# Import our new framework components\n",
    "from src import (\n",
    "    # Core data structures\n",
    "    DataConfig, TrainingConfig, GranularityLevel,\n",
    "    ModelMetadata, BenchmarkModel, ModelRegistry,\n",
    "    \n",
    "    # Main classes\n",
    "    DataLoader, FeatureEngineer, ModelTrainer,\n",
    "    ModelEvaluator, VisualizationGenerator,\n",
    "    \n",
    "    # Pipeline orchestration\n",
    "    BenchmarkPipeline\n",
    ")\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"Framework imported successfully!\")\n",
    "print(f\"Polars version: {pl.__version__}\")\n",
    "print(f\"XGBoost version: {xgb.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration Setup\n",
    "\n",
    "Instead of hardcoding paths and parameters like in the original notebook, we'll use configuration classes for better organization and reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration setup complete!\n",
      "Data directory: data\n",
      "Files exist: Features=True, Target=True, Mapping=True\n",
      "Lag features: [1, 2, 3, 4, 5, 6, 7]\n",
      "Training trials: 20\n"
     ]
    }
   ],
   "source": [
    "# Configure data loading (replaces the hardcoded paths in original notebook)\n",
    "from pathlib import Path\n",
    "\n",
    "# FIXED: Add data file existence checks\n",
    "data_dir = Path(\"data\")\n",
    "features_path = data_dir / \"train_data_features.feather\"\n",
    "target_path = data_dir / \"train_data_target.feather\"\n",
    "mapping_path = data_dir / \"feature_mapping_train.pkl\"\n",
    "\n",
    "# Verify paths exist\n",
    "missing_files = []\n",
    "for path in [features_path, target_path, mapping_path]:\n",
    "    if not path.exists():\n",
    "        missing_files.append(str(path))\n",
    "\n",
    "if missing_files:\n",
    "    print(f\"⚠️ Warning: The following data files were not found: {missing_files}\")\n",
    "    print(\"Note: This is expected if you haven't prepared the M5 dataset yet.\")\n",
    "    print(\"The notebook will demonstrate the framework structure even without actual data files.\")\n",
    "\n",
    "data_config = DataConfig(\n",
    "    features_path=str(features_path),\n",
    "    target_path=str(target_path), \n",
    "    mapping_path=str(mapping_path),\n",
    "    date_column=\"date\",\n",
    "    target_column=\"target\",\n",
    "    bdid_column=\"bdID\",\n",
    "    \n",
    "    # Feature engineering configuration (matches original notebook)\n",
    "    remove_not_for_sale=True,\n",
    "    lag_features=[1, 2, 3, 4, 5, 6, 7],  # Same as notebook\n",
    "    calendric_features=True,\n",
    "    trend_features=True\n",
    ")\n",
    "\n",
    "# Configure model training (replaces scattered parameters in notebook)\n",
    "training_config = TrainingConfig(\n",
    "    validation_split=0.2,  # Same 80/20 split as notebook\n",
    "    random_state=42,\n",
    "    n_trials=20,  # Reduced for demo (notebook used 50)\n",
    "    cv_folds=5,   # For time series cross-validation\n",
    "    model_type=\"xgboost\"\n",
    ")\n",
    "\n",
    "print(\"Configuration setup complete!\")\n",
    "print(f\"Data directory: {data_dir}\")\n",
    "print(f\"Files exist: Features={features_path.exists()}, Target={target_path.exists()}, Mapping={mapping_path.exists()}\")\n",
    "print(f\"Lag features: {data_config.lag_features}\")\n",
    "print(f\"Training trials: {training_config.n_trials}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading with DataLoader Class\n",
    "\n",
    "This replaces the manual data loading steps from the original notebook with a structured, reusable approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-22 21:42:52,310 - INFO - Loading M5 dataset...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading M5 dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-22 21:43:18,706 - INFO - Data loading completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (46881677, 53)\n",
      "Target shape: (59181090, 9)\n",
      "Feature mapping contains 40 mappings\n",
      "\n",
      "Features preview:\n",
      "shape: (5, 53)\n",
      "┌───────────┬──────┬───────────┬────────────┬───┬────────────┬────────────┬────────────┬───────────┐\n",
      "│ frequency ┆ idx  ┆ bdID      ┆ base_date  ┆ … ┆ feature_00 ┆ feature_00 ┆ feature_00 ┆ feature_0 │\n",
      "│ ---       ┆ ---  ┆ ---       ┆ ---        ┆   ┆ 36         ┆ 37         ┆ 38         ┆ 039       │\n",
      "│ str       ┆ i64  ┆ i64       ┆ date       ┆   ┆ ---        ┆ ---        ┆ ---        ┆ ---       │\n",
      "│           ┆      ┆           ┆            ┆   ┆ f64        ┆ f64        ┆ f64        ┆ f64       │\n",
      "╞═══════════╪══════╪═══════════╪════════════╪═══╪════════════╪════════════╪════════════╪═══════════╡\n",
      "│ daily     ┆ 1107 ┆ 231700515 ┆ 2014-02-08 ┆ … ┆ 0.0        ┆ 0.0        ┆ 0.0        ┆ 7.88      │\n",
      "│ daily     ┆ 1108 ┆ 231731005 ┆ 2014-02-09 ┆ … ┆ 0.0        ┆ 0.0        ┆ 0.0        ┆ 7.88      │\n",
      "│ daily     ┆ 1109 ┆ 231761495 ┆ 2014-02-10 ┆ … ┆ 0.0        ┆ 0.0        ┆ 1.0        ┆ 7.88      │\n",
      "│ daily     ┆ 1110 ┆ 231791985 ┆ 2014-02-11 ┆ … ┆ 0.0        ┆ 0.0        ┆ 2.0        ┆ 7.88      │\n",
      "│ daily     ┆ 1111 ┆ 231822475 ┆ 2014-02-12 ┆ … ┆ 0.0        ┆ 0.0        ┆ 0.0        ┆ 7.88      │\n",
      "└───────────┴──────┴───────────┴────────────┴───┴────────────┴────────────┴────────────┴───────────┘\n",
      "\n",
      "Target preview:\n",
      "shape: (5, 9)\n",
      "┌───────────┬─────┬───────────┬────────────┬───┬────────┬───────────────┬──────────────┬────────┐\n",
      "│ frequency ┆ idx ┆ bdID      ┆ base_date  ┆ … ┆ dateID ┆ missing_value ┆ not_for_sale ┆ target │\n",
      "│ ---       ┆ --- ┆ ---       ┆ ---        ┆   ┆ ---    ┆ ---           ┆ ---          ┆ ---    │\n",
      "│ str       ┆ i64 ┆ i64       ┆ date       ┆   ┆ i64    ┆ f64           ┆ f64          ┆ f64    │\n",
      "╞═══════════╪═════╪═══════════╪════════════╪═══╪════════╪═══════════════╪══════════════╪════════╡\n",
      "│ daily     ┆ 1   ┆ 197978575 ┆ 2011-01-29 ┆ … ┆ 4047   ┆ 0.0           ┆ 1.0          ┆ 0.0    │\n",
      "│ daily     ┆ 2   ┆ 198009065 ┆ 2011-01-30 ┆ … ┆ 4048   ┆ 0.0           ┆ 1.0          ┆ 0.0    │\n",
      "│ daily     ┆ 3   ┆ 198039555 ┆ 2011-01-31 ┆ … ┆ 4049   ┆ 0.0           ┆ 1.0          ┆ 0.0    │\n",
      "│ daily     ┆ 4   ┆ 198070045 ┆ 2011-02-01 ┆ … ┆ 4050   ┆ 0.0           ┆ 1.0          ┆ 0.0    │\n",
      "│ daily     ┆ 5   ┆ 198100535 ┆ 2011-02-02 ┆ … ┆ 4051   ┆ 0.0           ┆ 1.0          ┆ 0.0    │\n",
      "└───────────┴─────┴───────────┴────────────┴───┴────────┴───────────────┴──────────────┴────────┘\n"
     ]
    }
   ],
   "source": [
    "# Initialize the DataLoader (replaces manual file loading)\n",
    "data_loader = DataLoader(data_config)\n",
    "\n",
    "# Load the base dataset (equivalent to the pickle.load and pl.read_ipc calls)\n",
    "print(\"Loading M5 dataset...\")\n",
    "features_df, target_df, feature_mapping = data_loader.load_data(lazy=False)\n",
    "\n",
    "print(f\"Features shape: {features_df.shape}\")\n",
    "print(f\"Target shape: {target_df.shape}\")\n",
    "print(f\"Feature mapping contains {len(feature_mapping)} mappings\")\n",
    "\n",
    "# Display first few rows (equivalent to .head() in original notebook)\n",
    "print(\"\\nFeatures preview:\")\n",
    "print(features_df.head())\n",
    "\n",
    "print(\"\\nTarget preview:\")\n",
    "print(target_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Get Unique Entities\n",
    "\n",
    "Discover available SKUs, products, and stores in the dataset. This replaces manual exploration in the original notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-22 21:47:09,977 - INFO - Found 30490 unique SKUs\n",
      "2025-07-22 21:47:09,979 - INFO - Found 3049 unique products\n",
      "2025-07-22 21:47:09,979 - INFO - Found 10 unique stores\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset entity counts:\n",
      "Unique SKUs: 30490\n",
      "Unique Products: 3049\n",
      "Unique Stores: 10\n",
      "\n",
      "Example SKU IDs: [275417, 270585, 262701, 260084, 267045]\n",
      "Example Product IDs: [79050, 80381, 80119, 81146, 79604]\n",
      "Example Store IDs: [1334, 1337, 1331]\n",
      "<class 'int'>\n",
      "\n",
      "Selected demo SKU ID: 282275\n"
     ]
    }
   ],
   "source": [
    "# Get unique entities (replaces manual filtering and counting)\n",
    "unique_entities = data_loader.get_unique_entities()\n",
    "\n",
    "print(\"Dataset entity counts:\")\n",
    "print(f\"Unique SKUs: {len(unique_entities['skuIDs'])}\")\n",
    "print(f\"Unique Products: {len(unique_entities['productIDs'])}\")\n",
    "print(f\"Unique Stores: {len(unique_entities['storeIDs'])}\")\n",
    "\n",
    "# Show some example entities\n",
    "print(f\"\\nExample SKU IDs: {unique_entities['skuIDs'][:5]}\")\n",
    "print(f\"Example Product IDs: {unique_entities['productIDs'][:5]}\")\n",
    "print(f\"Example Store IDs: {unique_entities['storeIDs'][:3]}\")\n",
    "\n",
    "# Select a specific SKU for demonstration (use first available SKU from data)\n",
    "#demo_sku_id = unique_entities['skuIDs'][0]  # Use first available SKU\n",
    "demo_sku_id = 282275\n",
    "print(type(demo_sku_id))\n",
    "print(f\"\\nSelected demo SKU ID: {demo_sku_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Filter Data by Granularity\n",
    "\n",
    "Get data for a specific SKU. This replaces the manual filtering steps in the original notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering data for SKU 282275...\n",
      "Filtered features shape: (1941, 53)\n",
      "Filtered target shape: (1941, 9)\n",
      "\n",
      "Filtered features preview:\n",
      "shape: (5, 53)\n",
      "┌───────────┬─────────┬───────────┬────────────┬───┬───────────┬───────────┬───────────┬───────────┐\n",
      "│ frequency ┆ idx     ┆ bdID      ┆ base_date  ┆ … ┆ feature_0 ┆ feature_0 ┆ feature_0 ┆ feature_0 │\n",
      "│ ---       ┆ ---     ┆ ---       ┆ ---        ┆   ┆ 036       ┆ 037       ┆ 038       ┆ 039       │\n",
      "│ str       ┆ i64     ┆ i64       ┆ date       ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ---       │\n",
      "│           ┆         ┆           ┆            ┆   ┆ f64       ┆ f64       ┆ f64       ┆ f64       │\n",
      "╞═══════════╪═════════╪═══════════╪════════════╪═══╪═══════════╪═══════════╪═══════════╪═══════════╡\n",
      "│ daily     ┆ 9326506 ┆ 197981707 ┆ 2011-01-29 ┆ … ┆ 0.0       ┆ 0.0       ┆ 36.0      ┆ 1.25      │\n",
      "│ daily     ┆ 9326507 ┆ 198012197 ┆ 2011-01-30 ┆ … ┆ 0.0       ┆ 0.0       ┆ 14.0      ┆ 1.25      │\n",
      "│ daily     ┆ 9326508 ┆ 198042687 ┆ 2011-01-31 ┆ … ┆ 0.0       ┆ 0.0       ┆ 13.0      ┆ 1.25      │\n",
      "│ daily     ┆ 9326509 ┆ 198073177 ┆ 2011-02-01 ┆ … ┆ 0.0       ┆ 0.0       ┆ 13.0      ┆ 1.25      │\n",
      "│ daily     ┆ 9326510 ┆ 198103667 ┆ 2011-02-02 ┆ … ┆ 0.0       ┆ 0.0       ┆ 0.0       ┆ 1.25      │\n",
      "└───────────┴─────────┴───────────┴────────────┴───┴───────────┴───────────┴───────────┴───────────┘\n",
      "\n",
      "Filtered target preview:\n",
      "shape: (5, 9)\n",
      "┌───────────┬─────────┬───────────┬────────────┬───┬────────┬──────────────┬──────────────┬────────┐\n",
      "│ frequency ┆ idx     ┆ bdID      ┆ base_date  ┆ … ┆ dateID ┆ missing_valu ┆ not_for_sale ┆ target │\n",
      "│ ---       ┆ ---     ┆ ---       ┆ ---        ┆   ┆ ---    ┆ e            ┆ ---          ┆ ---    │\n",
      "│ str       ┆ i64     ┆ i64       ┆ date       ┆   ┆ i64    ┆ ---          ┆ f64          ┆ f64    │\n",
      "│           ┆         ┆           ┆            ┆   ┆        ┆ f64          ┆              ┆        │\n",
      "╞═══════════╪═════════╪═══════════╪════════════╪═══╪════════╪══════════════╪══════════════╪════════╡\n",
      "│ daily     ┆ 9326506 ┆ 197981707 ┆ 2011-01-29 ┆ … ┆ 4047   ┆ 0.0          ┆ 0.0          ┆ 36.0   │\n",
      "│ daily     ┆ 9326507 ┆ 198012197 ┆ 2011-01-30 ┆ … ┆ 4048   ┆ 0.0          ┆ 0.0          ┆ 14.0   │\n",
      "│ daily     ┆ 9326508 ┆ 198042687 ┆ 2011-01-31 ┆ … ┆ 4049   ┆ 0.0          ┆ 0.0          ┆ 13.0   │\n",
      "│ daily     ┆ 9326509 ┆ 198073177 ┆ 2011-02-01 ┆ … ┆ 4050   ┆ 0.0          ┆ 0.0          ┆ 13.0   │\n",
      "│ daily     ┆ 9326510 ┆ 198103667 ┆ 2011-02-02 ┆ … ┆ 4051   ┆ 0.0          ┆ 0.0          ┆ 0.0    │\n",
      "└───────────┴─────────┴───────────┴────────────┴───┴────────┴──────────────┴──────────────┴────────┘\n",
      "\n",
      "Date range: 2011-01-29 to 2016-05-22\n"
     ]
    }
   ],
   "source": [
    "# Filter data for specific SKU (replaces manual filtering in notebook)\n",
    "print(f\"Filtering data for SKU {demo_sku_id}...\")\n",
    "\n",
    "sku_features, sku_target = data_loader.get_data_for_granularity(\n",
    "    granularity=GranularityLevel.SKU,\n",
    "    entity_ids={\"skuID\": demo_sku_id},\n",
    "    collect=True\n",
    ")\n",
    "\n",
    "print(f\"Filtered features shape: {sku_features.shape}\")\n",
    "print(f\"Filtered target shape: {sku_target.shape}\")\n",
    "\n",
    "# Show data preview\n",
    "print(\"\\nFiltered features preview:\")\n",
    "print(sku_features.head())\n",
    "\n",
    "print(\"\\nFiltered target preview:\")\n",
    "print(sku_target.head())\n",
    "\n",
    "# Check date range\n",
    "date_min = sku_features.select(\"date\").min().item()\n",
    "date_max = sku_features.select(\"date\").max().item()\n",
    "print(f\"\\nDate range: {date_min} to {date_max}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Engineering\n",
    "\n",
    "Create features using the FeatureEngineer class. This systematizes the feature creation process from the original notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-22 21:47:16,509 - INFO - Creating features for sku level\n",
      "2025-07-22 21:47:16,523 - INFO - Created 138 features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating features...\n",
      "This includes:\n",
      "- Calendric features (month, day_of_week, quarter, etc.)\n",
      "- Lag features for temporal dependencies\n",
      "- Trend features\n",
      "- Automatic dummy encoding\n",
      "\n",
      "Engineered dataset shape: (1941, 151)\n",
      "Number of features created: 138\n",
      "\n",
      "Feature columns (first 10): ['lag_target_1', 'feature_0000', 'feature_0001', 'feature_0002', 'feature_0003', 'feature_0004', 'feature_0005', 'feature_0006', 'feature_0007', 'feature_0008', 'feature_0009', 'feature_0010', 'feature_0011', 'feature_0012', 'feature_0013', 'feature_0014', 'feature_0015', 'feature_0016', 'feature_0017', 'feature_0018', 'feature_0019', 'feature_0020', 'feature_0021', 'feature_0022', 'feature_0023', 'feature_0024', 'feature_0025', 'feature_0026', 'feature_0027', 'feature_0028', 'feature_0029', 'feature_0030', 'feature_0031', 'feature_0032', 'feature_0033', 'feature_0034', 'feature_0035', 'feature_0036', 'feature_0037', 'feature_0039', 'month_1', 'month_10', 'month_11', 'month_12', 'month_2', 'month_3', 'month_4', 'month_5', 'month_6', 'month_7', 'month_8', 'month_9', 'day_of_week_1', 'day_of_week_2', 'day_of_week_3', 'day_of_week_4', 'day_of_week_5', 'day_of_week_6', 'day_of_week_7', 'week_of_year_1', 'week_of_year_10', 'week_of_year_11', 'week_of_year_12', 'week_of_year_13', 'week_of_year_14', 'week_of_year_15', 'week_of_year_16', 'week_of_year_17', 'week_of_year_18', 'week_of_year_19', 'week_of_year_2', 'week_of_year_20', 'week_of_year_21', 'week_of_year_22', 'week_of_year_23', 'week_of_year_24', 'week_of_year_25', 'week_of_year_26', 'week_of_year_27', 'week_of_year_28', 'week_of_year_29', 'week_of_year_3', 'week_of_year_30', 'week_of_year_31', 'week_of_year_32', 'week_of_year_33', 'week_of_year_34', 'week_of_year_35', 'week_of_year_36', 'week_of_year_37', 'week_of_year_38', 'week_of_year_39', 'week_of_year_4', 'week_of_year_40', 'week_of_year_41', 'week_of_year_42', 'week_of_year_43', 'week_of_year_44', 'week_of_year_45', 'week_of_year_46', 'week_of_year_47', 'week_of_year_48', 'week_of_year_49', 'week_of_year_5', 'week_of_year_50', 'week_of_year_51', 'week_of_year_52', 'week_of_year_53', 'week_of_year_6', 'week_of_year_7', 'week_of_year_8', 'week_of_year_9', 'year_2011', 'year_2012', 'year_2013', 'year_2014', 'year_2015', 'year_2016', 'is_weekend_false', 'is_weekend_true', 'quarter_1', 'quarter_2', 'quarter_3', 'quarter_4', 'feature_0038_lag_1', 'feature_0038_lag_2', 'feature_0038_lag_3', 'feature_0038_lag_4', 'feature_0038_lag_5', 'feature_0038_lag_6', 'feature_0038_lag_7', 'target_lag_2', 'target_lag_3', 'target_lag_4', 'target_lag_5', 'target_lag_6', 'target_lag_7', 'trend']\n"
     ]
    }
   ],
   "source": [
    "# Initialize feature engineer (packages feature creation from notebook)\n",
    "feature_engineer = FeatureEngineer(\n",
    "    lag_features=data_config.lag_features,\n",
    "    calendric_features=data_config.calendric_features,\n",
    "    trend_features=data_config.trend_features\n",
    ")\n",
    "\n",
    "print(\"Creating features...\")\n",
    "print(\"This includes:\")\n",
    "print(\"- Calendric features (month, day_of_week, quarter, etc.)\")\n",
    "print(\"- Lag features for temporal dependencies\")\n",
    "print(\"- Trend features\")\n",
    "print(\"- Automatic dummy encoding\")\n",
    "\n",
    "# Create features (equivalent to all the feature engineering in notebook)\n",
    "engineered_df, feature_cols = feature_engineer.create_features(\n",
    "    sku_features, \n",
    "    sku_target,\n",
    "    granularity=GranularityLevel.SKU,\n",
    "    entity_ids={\"skuID\": demo_sku_id}\n",
    ")\n",
    "\n",
    "print(f\"\\nEngineered dataset shape: {engineered_df.shape}\")\n",
    "print(f\"Number of features created: {len(feature_cols)}\")\n",
    "print(f\"\\nFeature columns (first 10): {feature_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Prepare Model Data\n",
    "\n",
    "Clean the data and prepare X, y datasets for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-22 21:47:19,473 - INFO - Prepared data: 1934 samples, 138 features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model data prepared:\n",
      "X shape: (1934, 140)\n",
      "y shape: (1934, 2)\n",
      "\n",
      "Feature columns in X: ['bdID', 'date', 'lag_target_1', 'feature_0000', 'feature_0001']...\n",
      "Target column in y: target\n",
      "\n",
      "Remaining null values in X: 0\n"
     ]
    }
   ],
   "source": [
    "# Prepare clean model data (handles null values, separates X and y)\n",
    "X, y = feature_engineer.prepare_model_data(\n",
    "    engineered_df, \n",
    "    feature_cols, \n",
    "    target_col=data_config.target_column\n",
    ")\n",
    "\n",
    "print(f\"Model data prepared:\")\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "\n",
    "print(f\"\\nFeature columns in X: {X.columns[:5]}...\")  # Show first 5\n",
    "print(f\"Target column in y: {y.columns[1]}\")\n",
    "\n",
    "# Check for any remaining null values\n",
    "null_counts = X.null_count().sum_horizontal().item()\n",
    "print(f\"\\nRemaining null values in X: {null_counts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Create Temporal Train/Validation Split\n",
    "\n",
    "Split the data maintaining chronological order, just like in the original notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-22 21:47:25,334 - INFO - Created temporal split: 1547 train, 387 validation\n",
      "2025-07-22 21:47:25,335 - INFO - Split date: 2015-05-02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporal split with 20.0% validation...\n",
      "Training samples: 1547\n",
      "Validation samples: 387\n",
      "Split date: 2015-05-02\n",
      "\n",
      "Train set: X(1547, 140), y(1547, 2)\n",
      "Validation set: X(387, 140), y(387, 2)\n"
     ]
    }
   ],
   "source": [
    "# Create temporal split (maintains time order like in notebook)\n",
    "print(f\"Creating temporal split with {training_config.validation_split*100}% validation...\")\n",
    "\n",
    "train_bdids, val_bdids, split_date = data_loader.create_temporal_split(\n",
    "    X, \n",
    "    validation_split=training_config.validation_split\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(train_bdids)}\")\n",
    "print(f\"Validation samples: {len(val_bdids)}\")\n",
    "print(f\"Split date: {split_date}\")\n",
    "\n",
    "# Create train/validation datasets\n",
    "X_train = X.filter(pl.col(\"bdID\").is_in(train_bdids))\n",
    "y_train = y.filter(pl.col(\"bdID\").is_in(train_bdids))\n",
    "X_val = X.filter(pl.col(\"bdID\").is_in(val_bdids))\n",
    "y_val = y.filter(pl.col(\"bdID\").is_in(val_bdids))\n",
    "\n",
    "print(f\"\\nTrain set: X{X_train.shape}, y{y_train.shape}\")\n",
    "print(f\"Validation set: X{X_val.shape}, y{y_val.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Training with Optuna Optimization\n",
    "\n",
    "Train XGBoost model with hyperparameter optimization. This systematizes the training process from the original notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-22 21:47:28,191 - INFO - Training xgboost model for sku level\n",
      "[I 2025-07-22 21:47:28,200] A new study created in memory with name: no-name-af0c57c8-7d59-4c01-982d-ede0dea942f6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training XGBoost model with Optuna optimization...\n",
      "Number of trials: 20\n",
      "Model type: xgboost\n",
      "This will take a few minutes...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-22 21:47:28,396] Trial 0 finished with value: 81.98191214470285 and parameters: {'n_estimators': 219, 'max_depth': 4, 'learning_rate': 0.05462624227063438, 'subsample': 0.9012363512836443, 'colsample_bytree': 0.9063499942120274, 'reg_alpha': 2.5524547843839427, 'reg_lambda': 7.150919719099195}. Best is trial 0 with value: 81.98191214470285.\n",
      "[I 2025-07-22 21:47:28,596] Trial 1 finished with value: 82.88372093023256 and parameters: {'n_estimators': 76, 'max_depth': 9, 'learning_rate': 0.05448557149352935, 'subsample': 0.9472470281240261, 'colsample_bytree': 0.7332246404452674, 'reg_alpha': 2.7235958065538246, 'reg_lambda': 9.729665952439591}. Best is trial 0 with value: 81.98191214470285.\n",
      "[I 2025-07-22 21:47:28,734] Trial 2 finished with value: 108.94315245478036 and parameters: {'n_estimators': 158, 'max_depth': 4, 'learning_rate': 0.2788959797647703, 'subsample': 0.9556100768868356, 'colsample_bytree': 0.8148846924019517, 'reg_alpha': 1.1844836622436872, 'reg_lambda': 8.839778956074193}. Best is trial 0 with value: 81.98191214470285.\n",
      "[I 2025-07-22 21:47:29,219] Trial 3 finished with value: 99.75968992248062 and parameters: {'n_estimators': 172, 'max_depth': 12, 'learning_rate': 0.07984106199730485, 'subsample': 0.9610798157558627, 'colsample_bytree': 0.909173726987147, 'reg_alpha': 3.7985672901809697, 'reg_lambda': 1.671016586556167}. Best is trial 0 with value: 81.98191214470285.\n",
      "[I 2025-07-22 21:47:29,592] Trial 4 finished with value: 97.9922480620155 and parameters: {'n_estimators': 224, 'max_depth': 8, 'learning_rate': 0.14143036868259182, 'subsample': 0.8838514817315165, 'colsample_bytree': 0.7985993403092255, 'reg_alpha': 7.188050028676972, 'reg_lambda': 1.0429219451212794}. Best is trial 0 with value: 81.98191214470285.\n",
      "[I 2025-07-22 21:47:30,558] Trial 5 finished with value: 100.62273901808786 and parameters: {'n_estimators': 281, 'max_depth': 17, 'learning_rate': 0.2224191915523793, 'subsample': 0.8528440212130012, 'colsample_bytree': 0.9110878068894982, 'reg_alpha': 2.5265924715881285, 'reg_lambda': 9.17613226572789}. Best is trial 0 with value: 81.98191214470285.\n",
      "[I 2025-07-22 21:47:31,566] Trial 6 finished with value: 102.10852713178295 and parameters: {'n_estimators': 278, 'max_depth': 19, 'learning_rate': 0.262014561908013, 'subsample': 0.8210248445686169, 'colsample_bytree': 0.9522408737387806, 'reg_alpha': 2.361226938775612, 'reg_lambda': 6.291614017219823}. Best is trial 0 with value: 81.98191214470285.\n",
      "[I 2025-07-22 21:47:31,672] Trial 7 finished with value: 109.02067183462532 and parameters: {'n_estimators': 79, 'max_depth': 6, 'learning_rate': 0.4572530221229873, 'subsample': 0.8825992505895566, 'colsample_bytree': 0.9793246684362789, 'reg_alpha': 9.473462378907696, 'reg_lambda': 7.744634962197179}. Best is trial 0 with value: 81.98191214470285.\n",
      "[I 2025-07-22 21:47:32,222] Trial 8 finished with value: 109.9250645994832 and parameters: {'n_estimators': 161, 'max_depth': 18, 'learning_rate': 0.2153911340358079, 'subsample': 0.8956037327130855, 'colsample_bytree': 0.8610067534772539, 'reg_alpha': 9.430971391785473, 'reg_lambda': 1.6781298273457692}. Best is trial 0 with value: 81.98191214470285.\n",
      "[I 2025-07-22 21:47:32,579] Trial 9 finished with value: 106.63049095607235 and parameters: {'n_estimators': 108, 'max_depth': 15, 'learning_rate': 0.22973446892668836, 'subsample': 0.839033851880337, 'colsample_bytree': 0.9910466452609299, 'reg_alpha': 5.474130647463244, 'reg_lambda': 1.6429495764845428}. Best is trial 0 with value: 81.98191214470285.\n",
      "[I 2025-07-22 21:47:32,696] Trial 10 finished with value: 83.36692506459949 and parameters: {'n_estimators': 224, 'max_depth': 2, 'learning_rate': 0.10304336144658031, 'subsample': 0.7312660771308257, 'colsample_bytree': 0.7096278355297116, 'reg_alpha': 5.577368657805467, 'reg_lambda': 4.536813438179715}. Best is trial 0 with value: 81.98191214470285.\n",
      "[I 2025-07-22 21:47:32,821] Trial 11 finished with value: 84.5297157622739 and parameters: {'n_estimators': 52, 'max_depth': 9, 'learning_rate': 0.05267639665812589, 'subsample': 0.9945317462611318, 'colsample_bytree': 0.7038670635205554, 'reg_alpha': 0.0077779266707564965, 'reg_lambda': 7.023521000148969}. Best is trial 0 with value: 81.98191214470285.\n",
      "[I 2025-07-22 21:47:33,422] Trial 12 finished with value: 91.37467700258398 and parameters: {'n_estimators': 221, 'max_depth': 12, 'learning_rate': 0.05329171014978306, 'subsample': 0.9293114506938441, 'colsample_bytree': 0.7682084927783962, 'reg_alpha': 3.662158516596813, 'reg_lambda': 9.996869229157282}. Best is trial 0 with value: 81.98191214470285.\n",
      "[I 2025-07-22 21:47:33,584] Trial 13 finished with value: 87.13695090439276 and parameters: {'n_estimators': 119, 'max_depth': 6, 'learning_rate': 0.08039361344484496, 'subsample': 0.7839977906424298, 'colsample_bytree': 0.8669667895440345, 'reg_alpha': 1.846155550265065, 'reg_lambda': 4.779674004525416}. Best is trial 0 with value: 81.98191214470285.\n",
      "[I 2025-07-22 21:47:33,695] Trial 14 finished with value: 81.77002583979328 and parameters: {'n_estimators': 203, 'max_depth': 2, 'learning_rate': 0.0720023991766942, 'subsample': 0.9231770270991924, 'colsample_bytree': 0.752257022353642, 'reg_alpha': 3.906456674923181, 'reg_lambda': 7.975063927567957}. Best is trial 14 with value: 81.77002583979328.\n",
      "[I 2025-07-22 21:47:33,827] Trial 15 finished with value: 82.68475452196382 and parameters: {'n_estimators': 248, 'max_depth': 2, 'learning_rate': 0.07358757182783676, 'subsample': 0.9157756340240906, 'colsample_bytree': 0.9117705087432255, 'reg_alpha': 4.247591128830933, 'reg_lambda': 8.038939972507563}. Best is trial 14 with value: 81.77002583979328.\n",
      "[I 2025-07-22 21:47:34,041] Trial 16 finished with value: 108.41343669250647 and parameters: {'n_estimators': 195, 'max_depth': 5, 'learning_rate': 0.1257291581276269, 'subsample': 0.9987359986659785, 'colsample_bytree': 0.8358387029332521, 'reg_alpha': 6.751004654714168, 'reg_lambda': 5.845568452706786}. Best is trial 14 with value: 81.77002583979328.\n",
      "[I 2025-07-22 21:47:34,182] Trial 17 finished with value: 90.8062015503876 and parameters: {'n_estimators': 197, 'max_depth': 3, 'learning_rate': 0.10095050910760343, 'subsample': 0.775772842319921, 'colsample_bytree': 0.7714220949540044, 'reg_alpha': 0.5823709523645952, 'reg_lambda': 3.825250288839206}. Best is trial 14 with value: 81.77002583979328.\n",
      "[I 2025-07-22 21:47:34,588] Trial 18 finished with value: 95.30749354005168 and parameters: {'n_estimators': 262, 'max_depth': 7, 'learning_rate': 0.06832209605551705, 'subsample': 0.8586563739870166, 'colsample_bytree': 0.9443828460710905, 'reg_alpha': 6.764574664548895, 'reg_lambda': 7.4059414172050575}. Best is trial 14 with value: 81.77002583979328.\n",
      "[I 2025-07-22 21:47:34,915] Trial 19 finished with value: 106.89922480620154 and parameters: {'n_estimators': 145, 'max_depth': 10, 'learning_rate': 0.1741817868813089, 'subsample': 0.8065830168900265, 'colsample_bytree': 0.8835161693546031, 'reg_alpha': 4.729931354909983, 'reg_lambda': 6.590154833234091}. Best is trial 14 with value: 81.77002583979328.\n",
      "2025-07-22 21:47:34,916 - INFO - Best parameters: {'n_estimators': 203, 'max_depth': 2, 'learning_rate': 0.0720023991766942, 'subsample': 0.9231770270991924, 'colsample_bytree': 0.752257022353642, 'reg_alpha': 3.906456674923181, 'reg_lambda': 7.975063927567957}\n",
      "2025-07-22 21:47:34,916 - INFO - Best validation MSE: 81.7700\n",
      "2025-07-22 21:47:35,022 - INFO - Model training completed. Validation MSE: 82.6408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "MODEL TRAINING COMPLETED\n",
      "==================================================\n",
      "Model ID: sku_282275_xgboost\n",
      "Best hyperparameters: {'n_estimators': 203, 'max_depth': 2, 'learning_rate': 0.0720023991766942, 'subsample': 0.9231770270991924, 'colsample_bytree': 0.752257022353642, 'reg_alpha': 3.906456674923181, 'reg_lambda': 7.975063927567957}\n",
      "Validation performance: {'mse': 82.64082687338501, 'rmse': np.float64(9.090700021086661), 'mae': 6.708010335917312, 'r2': 0.5069528342744637, 'mape': np.float64(42.87625195151883)}\n"
     ]
    }
   ],
   "source": [
    "# Initialize model trainer (packages the training logic from notebook)\n",
    "model_trainer = ModelTrainer(training_config)\n",
    "\n",
    "print(f\"Training XGBoost model with Optuna optimization...\")\n",
    "print(f\"Number of trials: {training_config.n_trials}\")\n",
    "print(f\"Model type: {training_config.model_type}\")\n",
    "print(f\"This will take a few minutes...\\n\")\n",
    "\n",
    "# Train model (equivalent to the Optuna optimization in notebook)\n",
    "trained_model = model_trainer.train_model(\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val,\n",
    "    feature_cols=feature_cols,\n",
    "    target_col=data_config.target_column,\n",
    "    granularity=GranularityLevel.SKU,\n",
    "    entity_ids={\"skuID\": demo_sku_id}\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MODEL TRAINING COMPLETED\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Model ID: {trained_model.get_identifier()}\")\n",
    "print(f\"Best hyperparameters: {trained_model.metadata.hyperparameters}\")\n",
    "print(f\"Validation performance: {trained_model.metadata.performance_metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Storage and Registry\n",
    "\n",
    "Save the model using the registry system for later retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved with ID: sku_282275_xgboost\n",
      "Storage location: test_models\n",
      "\n",
      "Model successfully loaded back from disk\n",
      "Loaded model type: xgboost\n",
      "Loaded model features: 138\n",
      "\n",
      "Models in registry: 1\n",
      "Model IDs: ['sku_282275_xgboost']\n"
     ]
    }
   ],
   "source": [
    "# Initialize model registry (new capability not in original notebook)\n",
    "registry = ModelRegistry(storage_path=Path(\"test_models\"))\n",
    "\n",
    "# Register and save model\n",
    "model_id = registry.register_model(trained_model)\n",
    "registry.save_model(model_id)\n",
    "\n",
    "print(f\"Model saved with ID: {model_id}\")\n",
    "print(f\"Storage location: {registry.storage_path}\")\n",
    "\n",
    "# Test loading the model back\n",
    "loaded_model = registry.load_model(model_id)\n",
    "print(f\"\\nModel successfully loaded back from disk\")\n",
    "print(f\"Loaded model type: {loaded_model.metadata.model_type}\")\n",
    "print(f\"Loaded model features: {len(loaded_model.metadata.feature_columns)}\")\n",
    "\n",
    "# List all models in registry\n",
    "all_model_ids = registry.list_models()\n",
    "print(f\"\\nModels in registry: {len(all_model_ids)}\")\n",
    "print(f\"Model IDs: {all_model_ids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Model Evaluation\n",
    "\n",
    "Comprehensive evaluation of the trained model using the evaluation framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-22 21:50:32,725 - INFO - Evaluating model with provided data: sku_282275_xgboost\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model performance...\n",
      "\n",
      "==================================================\n",
      "EVALUATION RESULTS\n",
      "==================================================\n",
      "Model ID: sku_282275_xgboost\n",
      "Granularity: sku\n",
      "Test samples: 387\n",
      "\n",
      "Performance Metrics:\n",
      "  MSE: 82.6408\n",
      "  RMSE: 9.0907\n",
      "  MAE: 6.7080\n",
      "  R2: 0.5070\n",
      "  MAPE: 42.8763\n",
      "  MAX_ERROR: 42.0000\n",
      "  MEAN_ERROR: -0.7028\n",
      "  STD_ERROR: 9.0635\n",
      "  WITHIN_1_UNIT: 14.9871\n",
      "  within_2_units: 25.84%\n",
      "  within_5_units: 54.52%\n"
     ]
    }
   ],
   "source": [
    "# Initialize evaluator (systematic evaluation not in original notebook)\n",
    "evaluator = ModelEvaluator(data_loader, registry)\n",
    "\n",
    "print(\"Evaluating model performance...\")\n",
    "\n",
    "# Comprehensive evaluation using the new method with pre-engineered data\n",
    "eval_result = evaluator.evaluate_model_with_data(loaded_model, X, y)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EVALUATION RESULTS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Model ID: {eval_result['model_id']}\")\n",
    "print(f\"Granularity: {eval_result['granularity']}\")\n",
    "print(f\"Test samples: {eval_result['n_samples']}\")\n",
    "\n",
    "# Print all metrics\n",
    "print(\"\\nPerformance Metrics:\")\n",
    "for metric, value in eval_result['metrics'].items():\n",
    "    if metric.endswith('_units'):\n",
    "        print(f\"  {metric}: {value:.2f}%\")\n",
    "    else:\n",
    "        print(f\"  {metric.upper()}: {value:.4f}\")\n",
    "\n",
    "# Feature importance (if available)\n",
    "if eval_result.get('feature_importance'):\n",
    "    print(\"\\nTop 10 Most Important Features:\")\n",
    "    for i, (feature, importance) in enumerate(list(eval_result['feature_importance'].items())[:10], 1):\n",
    "        print(f\"  {i:2d}. {feature}: {importance:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Create Visualizations\n",
    "\n",
    "Generate the same style of plots as in the original notebook using the visualization framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div id=\"nUNlqi\"></div>\n",
       "            <script type=\"text/javascript\" data-lets-plot-script=\"library\">\n",
       "                if(!window.letsPlotCallQueue) {\n",
       "                    window.letsPlotCallQueue = [];\n",
       "                }; \n",
       "                window.letsPlotCall = function(f) {\n",
       "                    window.letsPlotCallQueue.push(f);\n",
       "                };\n",
       "                (function() {\n",
       "                    var script = document.createElement(\"script\");\n",
       "                    script.type = \"text/javascript\";\n",
       "                    script.src = \"https://cdn.jsdelivr.net/gh/JetBrains/lets-plot@v4.6.2/js-package/distr/lets-plot.min.js\";\n",
       "                    script.onload = function() {\n",
       "                        window.letsPlotCall = function(f) {f();};\n",
       "                        window.letsPlotCallQueue.forEach(function(f) {f();});\n",
       "                        window.letsPlotCallQueue = [];\n",
       "                        \n",
       "                    };\n",
       "                    script.onerror = function(event) {\n",
       "                        window.letsPlotCall = function(f) {};    // noop\n",
       "                        window.letsPlotCallQueue = [];\n",
       "                        var div = document.createElement(\"div\");\n",
       "                        div.style.color = 'darkred';\n",
       "                        div.textContent = 'Error loading Lets-Plot JS';\n",
       "                        document.getElementById(\"nUNlqi\").appendChild(div);\n",
       "                    };\n",
       "                    var e = document.getElementById(\"nUNlqi\");\n",
       "                    e.appendChild(script);\n",
       "                })()\n",
       "            </script>\n",
       "            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating visualizations...\n",
      "\n",
      "1. Predictions vs Actuals Plot:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "   <div id=\"xJbF1R\"></div>\n",
       "   <script type=\"text/javascript\" data-lets-plot-script=\"plot\">\n",
       "   \n",
       "   (function() {\n",
       "   // ----------\n",
       "   \n",
       "   const forceImmediateRender = false;\n",
       "   const responsive = false;\n",
       "   \n",
       "   let sizing = {\n",
       "       width_mode: \"MIN\",\n",
       "       height_mode: \"SCALED\",\n",
       "       width: null, \n",
       "       height: null \n",
       "   };\n",
       "   \n",
       "   const preferredWidth = document.body.dataset.letsPlotPreferredWidth;\n",
       "   if (preferredWidth !== undefined) {\n",
       "       sizing = {\n",
       "           width_mode: 'FIXED',\n",
       "           height_mode: 'SCALED',\n",
       "           width: parseFloat(preferredWidth)\n",
       "       };\n",
       "   }\n",
       "   \n",
       "   const containerDiv = document.getElementById(\"xJbF1R\");\n",
       "   let fig = null;\n",
       "   \n",
       "   function renderPlot() {\n",
       "       if (fig === null) {\n",
       "           const plotSpec = {\n",
       "\"data\":{\n",
       "\"actual\":[1.0,1.0,0.0,0.0,3.0,10.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,1.0,2.0,1.0,0.0,0.0,0.0,2.0,0.0,3.0,0.0,0.0,0.0,1.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,3.0,2.0,1.0,0.0,3.0,0.0,0.0,1.0,0.0,2.0,1.0,0.0,0.0,0.0,0.0,2.0,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,1.0,1.0,1.0,0.0,1.0,2.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,1.0,3.0,0.0,0.0,1.0,0.0,1.0,0.0,0.0,0.0,0.0,1.0,1.0,1.0,1.0,1.0,0.0,0.0,1.0,0.0,1.0,1.0,0.0,0.0,0.0,0.0,6.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,1.0,1.0,0.0,0.0,1.0,1.0,4.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,1.0,0.0,1.0,3.0,0.0,0.0,0.0,1.0,1.0],\n",
       "\"predicted\":[1.0,0.0,0.0,1.0,0.0,1.0,0.0,0.0,1.0,1.0,0.0,0.0,1.0,0.0,0.0,1.0,0.0,0.0,1.0,0.0,1.0,0.0,1.0,1.0,0.0,1.0,0.0,0.0,0.0,0.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,0.0,0.0,1.0,1.0,0.0,0.0,1.0,0.0,1.0,0.0,0.0,1.0,0.0,0.0,1.0,0.0,1.0,1.0,0.0,0.0,1.0,1.0,1.0,1.0,0.0,0.0,1.0,1.0,0.0,1.0,0.0,0.0,1.0,0.0,0.0,1.0,1.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,1.0,0.0,0.0,0.0,1.0,0.0,1.0,1.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,1.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,0.0,1.0,0.0,0.0,0.0]\n",
       "},\n",
       "\"mapping\":{\n",
       "\"x\":\"actual\",\n",
       "\"y\":\"predicted\"\n",
       "},\n",
       "\"data_meta\":{\n",
       "\"series_annotations\":[{\n",
       "\"type\":\"float\",\n",
       "\"column\":\"actual\"\n",
       "},{\n",
       "\"type\":\"int\",\n",
       "\"column\":\"predicted\"\n",
       "}]\n",
       "},\n",
       "\"ggtitle\":{\n",
       "\"text\":\"Predictions vs Actuals: sku_276349_xgboost\"\n",
       "},\n",
       "\"guides\":{\n",
       "\"x\":{\n",
       "\"title\":\"Actual Values\"\n",
       "},\n",
       "\"y\":{\n",
       "\"title\":\"Predicted Values\"\n",
       "}\n",
       "},\n",
       "\"theme\":{\n",
       "\"name\":\"minimal\"\n",
       "},\n",
       "\"kind\":\"plot\",\n",
       "\"scales\":[],\n",
       "\"layers\":[{\n",
       "\"geom\":\"point\",\n",
       "\"mapping\":{\n",
       "},\n",
       "\"data_meta\":{\n",
       "},\n",
       "\"alpha\":0.6,\n",
       "\"data\":{\n",
       "}\n",
       "}],\n",
       "\"metainfo_list\":[],\n",
       "\"spec_id\":\"1\"\n",
       "};\n",
       "           window.letsPlotCall(function() { fig = LetsPlot.buildPlotFromProcessedSpecs(plotSpec, containerDiv, sizing); });\n",
       "       } else {\n",
       "           fig.updateView({});\n",
       "       }\n",
       "   }\n",
       "   \n",
       "   const renderImmediately = \n",
       "       forceImmediateRender || (\n",
       "           sizing.width_mode === 'FIXED' && \n",
       "           (sizing.height_mode === 'FIXED' || sizing.height_mode === 'SCALED')\n",
       "       );\n",
       "   \n",
       "   if (renderImmediately) {\n",
       "       renderPlot();\n",
       "   }\n",
       "   \n",
       "   if (!renderImmediately || responsive) {\n",
       "       // Set up observer for initial sizing or continuous monitoring\n",
       "       var observer = new ResizeObserver(function(entries) {\n",
       "           for (let entry of entries) {\n",
       "               if (entry.contentBoxSize && \n",
       "                   entry.contentBoxSize[0].inlineSize > 0) {\n",
       "                   if (!responsive && observer) {\n",
       "                       observer.disconnect();\n",
       "                       observer = null;\n",
       "                   }\n",
       "                   renderPlot();\n",
       "                   if (!responsive) {\n",
       "                       break;\n",
       "                   }\n",
       "               }\n",
       "           }\n",
       "       });\n",
       "       \n",
       "       observer.observe(containerDiv);\n",
       "   }\n",
       "   \n",
       "   // ----------\n",
       "   })();\n",
       "   \n",
       "   </script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. Error Distribution Plot:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "   <div id=\"7onfGd\"></div>\n",
       "   <script type=\"text/javascript\" data-lets-plot-script=\"plot\">\n",
       "   \n",
       "   (function() {\n",
       "   // ----------\n",
       "   \n",
       "   const forceImmediateRender = false;\n",
       "   const responsive = false;\n",
       "   \n",
       "   let sizing = {\n",
       "       width_mode: \"MIN\",\n",
       "       height_mode: \"SCALED\",\n",
       "       width: null, \n",
       "       height: null \n",
       "   };\n",
       "   \n",
       "   const preferredWidth = document.body.dataset.letsPlotPreferredWidth;\n",
       "   if (preferredWidth !== undefined) {\n",
       "       sizing = {\n",
       "           width_mode: 'FIXED',\n",
       "           height_mode: 'SCALED',\n",
       "           width: parseFloat(preferredWidth)\n",
       "       };\n",
       "   }\n",
       "   \n",
       "   const containerDiv = document.getElementById(\"7onfGd\");\n",
       "   let fig = null;\n",
       "   \n",
       "   function renderPlot() {\n",
       "       if (fig === null) {\n",
       "           const plotSpec = {\n",
       "\"data\":{\n",
       "},\n",
       "\"mapping\":{\n",
       "\"x\":\"errors\"\n",
       "},\n",
       "\"data_meta\":{\n",
       "\"series_annotations\":[{\n",
       "\"type\":\"float\",\n",
       "\"column\":\"errors\"\n",
       "}]\n",
       "},\n",
       "\"ggtitle\":{\n",
       "\"text\":\"Prediction Error Distribution: sku_276349_xgboost\"\n",
       "},\n",
       "\"guides\":{\n",
       "\"x\":{\n",
       "\"title\":\"Prediction Error\"\n",
       "},\n",
       "\"y\":{\n",
       "\"title\":\"Frequency\"\n",
       "}\n",
       "},\n",
       "\"theme\":{\n",
       "\"name\":\"minimal\"\n",
       "},\n",
       "\"kind\":\"plot\",\n",
       "\"scales\":[],\n",
       "\"layers\":[{\n",
       "\"geom\":\"histogram\",\n",
       "\"mapping\":{\n",
       "},\n",
       "\"data_meta\":{\n",
       "},\n",
       "\"bins\":50.0,\n",
       "\"alpha\":0.7,\n",
       "\"data\":{\n",
       "\"..count..\":[32.0,0.0,0.0,0.0,0.0,72.0,0.0,0.0,0.0,0.0,23.0,0.0,0.0,0.0,0.0,7.0,0.0,0.0,0.0,0.0,4.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0],\n",
       "\"errors\":[-1.0372,-0.8315999999999999,-0.6259999999999999,-0.4204,-0.2148,-0.009200000000000097,0.1963999999999999,0.4019999999999999,0.6075999999999999,0.8131999999999999,1.0187999999999997,1.2243999999999997,1.4299999999999997,1.6355999999999997,1.8411999999999997,2.0467999999999997,2.2523999999999997,2.4579999999999997,2.6635999999999997,2.8691999999999998,3.0747999999999993,3.2803999999999998,3.4859999999999993,3.6915999999999998,3.8971999999999993,4.1028,4.308399999999999,4.513999999999999,4.7196,4.9252,5.130799999999999,5.336399999999999,5.542,5.7475999999999985,5.953199999999999,6.158799999999999,6.3644,6.5699999999999985,6.775599999999999,6.981199999999999,7.186799999999998,7.3923999999999985,7.597999999999999,7.803599999999999,8.009199999999998,8.214799999999999,8.420399999999999,8.626,8.831599999999998,9.037199999999999]\n",
       "}\n",
       "}],\n",
       "\"metainfo_list\":[],\n",
       "\"spec_id\":\"2\"\n",
       "};\n",
       "           window.letsPlotCall(function() { fig = LetsPlot.buildPlotFromProcessedSpecs(plotSpec, containerDiv, sizing); });\n",
       "       } else {\n",
       "           fig.updateView({});\n",
       "       }\n",
       "   }\n",
       "   \n",
       "   const renderImmediately = \n",
       "       forceImmediateRender || (\n",
       "           sizing.width_mode === 'FIXED' && \n",
       "           (sizing.height_mode === 'FIXED' || sizing.height_mode === 'SCALED')\n",
       "       );\n",
       "   \n",
       "   if (renderImmediately) {\n",
       "       renderPlot();\n",
       "   }\n",
       "   \n",
       "   if (!renderImmediately || responsive) {\n",
       "       // Set up observer for initial sizing or continuous monitoring\n",
       "       var observer = new ResizeObserver(function(entries) {\n",
       "           for (let entry of entries) {\n",
       "               if (entry.contentBoxSize && \n",
       "                   entry.contentBoxSize[0].inlineSize > 0) {\n",
       "                   if (!responsive && observer) {\n",
       "                       observer.disconnect();\n",
       "                       observer = null;\n",
       "                   }\n",
       "                   renderPlot();\n",
       "                   if (!responsive) {\n",
       "                       break;\n",
       "                   }\n",
       "               }\n",
       "           }\n",
       "       });\n",
       "       \n",
       "       observer.observe(containerDiv);\n",
       "   }\n",
       "   \n",
       "   // ----------\n",
       "   })();\n",
       "   \n",
       "   </script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Visualization plots displayed above!\n",
      "\n",
      "Prediction Statistics:\n",
      "Predictions - Mean: 0.38, Std: 0.49, Range: [0, 1]\n",
      "Actuals - Mean: 0.64, Std: 1.23, Range: [0, 10]\n",
      "Absolute errors - Mean: 0.71, Max: 9\n"
     ]
    }
   ],
   "source": [
    "# Initialize visualization generator\n",
    "viz_gen = VisualizationGenerator()\n",
    "\n",
    "print(\"Creating visualizations...\")\n",
    "\n",
    "if viz_gen.lets_plot_available:\n",
    "    # Create prediction vs actual plot (like in notebook)\n",
    "    print(\"\\n1. Predictions vs Actuals Plot:\")\n",
    "    pred_plot = viz_gen.create_prediction_plot(eval_result)\n",
    "    if pred_plot:\n",
    "        pred_plot.show()\n",
    "    \n",
    "    # Create error distribution plot (like in notebook)\n",
    "    print(\"\\n2. Error Distribution Plot:\")\n",
    "    error_plot = viz_gen.create_error_distribution_plot(eval_result)\n",
    "    if error_plot:\n",
    "        error_plot.show()\n",
    "    \n",
    "    print(\"\\nVisualization plots displayed above!\")\n",
    "    \n",
    "else:\n",
    "    print(\"lets-plot not available for visualization\")\n",
    "    \n",
    "# Show basic statistics about predictions vs actuals\n",
    "predictions = eval_result['predictions']\n",
    "actuals = eval_result['actuals']\n",
    "\n",
    "# Convert lists to numpy arrays for mathematical operations\n",
    "predictions_array = np.array(predictions)\n",
    "actuals_array = np.array(actuals)\n",
    "\n",
    "print(f\"\\nPrediction Statistics:\")\n",
    "print(f\"Predictions - Mean: {np.mean(predictions_array):.2f}, Std: {np.std(predictions_array):.2f}, Range: [{np.min(predictions_array):.0f}, {np.max(predictions_array):.0f}]\")\n",
    "print(f\"Actuals - Mean: {np.mean(actuals_array):.2f}, Std: {np.std(actuals_array):.2f}, Range: [{np.min(actuals_array):.0f}, {np.max(actuals_array):.0f}]\")\n",
    "print(f\"Absolute errors - Mean: {np.mean(np.abs(actuals_array - predictions_array)):.2f}, Max: {np.max(np.abs(actuals_array - predictions_array)):.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Generate Evaluation Report\n",
    "\n",
    "Create a comprehensive markdown report (new capability)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-22 21:45:10,670 - INFO - Report saved to test_models/evaluation_report.md\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation report generated and saved to: test_models/evaluation_report.md\n",
      "\n",
      "============================================================\n",
      "EVALUATION REPORT PREVIEW\n",
      "============================================================\n",
      "# Model Evaluation Report\n",
      "## Model: sku_276349_xgboost\n",
      "**Granularity:** sku\n",
      "**Entity IDs:** {'skuID': 276349}\n",
      "**Test Samples:** 141\n",
      "\n",
      "### Performance Metrics\n",
      "- **MSE:** 1.7872\n",
      "- **RMSE:** 1.3369\n",
      "- **MAE:** 0.7092\n",
      "- **R2:** -0.1856\n",
      "- **MAPE:** 64.9697\n",
      "- **MAX_ERROR:** 9.0000\n",
      "- **MEAN_ERROR:** 0.2553\n",
      "- **STD_ERROR:** 1.3123\n",
      "- **WITHIN_1_UNIT:** 90.0709\n",
      "- **WITHIN_2_UNITS:** 95.0355\n",
      "- **WITHIN_5_UNITS:** 98.5816\n",
      "\n",
      "\n",
      "\n",
      "[Report continues in the saved file...]\n"
     ]
    }
   ],
   "source": [
    "# Generate comprehensive report\n",
    "report = evaluator.generate_evaluation_report(\n",
    "    eval_result,\n",
    "    output_path=Path(\"test_models/evaluation_report.md\")\n",
    ")\n",
    "\n",
    "print(\"Evaluation report generated and saved to: test_models/evaluation_report.md\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATION REPORT PREVIEW\")\n",
    "print(\"=\"*60)\n",
    "print(report[:1000] + \"...\" if len(report) > 1000 else report)\n",
    "print(\"\\n[Report continues in the saved file...]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Comparison with Original Notebook Approach\n",
    "\n",
    "Let's compare our framework results with a quick manual approach similar to the original notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training a quick comparison model using manual approach...\n",
      "\n",
      "============================================================\n",
      "FRAMEWORK vs MANUAL APPROACH COMPARISON\n",
      "============================================================\n",
      "Framework (Optimized) Results:\n",
      "  RMSE: 0.2236\n",
      "  R²: 0.9782\n",
      "  Hyperparameter trials: 20\n",
      "\n",
      "Manual (Basic) Results:\n",
      "  RMSE: 0.0000\n",
      "  R²: 1.0000\n",
      "  Hyperparameter trials: 0 (fixed parameters)\n",
      "\n",
      "Framework Improvement: -inf% better RMSE\n",
      "\n",
      "Framework Advantages:\n",
      "+ Systematic hyperparameter optimization\n",
      "+ Automatic model storage and metadata tracking\n",
      "+ Comprehensive evaluation metrics\n",
      "+ Reproducible configuration management\n",
      "+ Easy scaling to multiple models/granularities\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/g5/xwl2l1dd18119_crmh57rc2r0000gn/T/ipykernel_99520/2949424773.py:40: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  improvement = ((manual_rmse - eval_result['metrics']['rmse']) / manual_rmse) * 100\n"
     ]
    }
   ],
   "source": [
    "# Quick manual model training (similar to notebook approach)\n",
    "print(\"Training a quick comparison model using manual approach...\")\n",
    "\n",
    "# Convert to numpy for direct XGBoost training\n",
    "X_train_np = X_train.select(feature_cols).to_numpy()\n",
    "y_train_np = y_train.select(data_config.target_column).to_numpy().flatten()\n",
    "X_val_np = X_val.select(feature_cols).to_numpy()\n",
    "y_val_np = y_val.select(data_config.target_column).to_numpy().flatten()\n",
    "\n",
    "# Simple XGBoost model (like original notebook without optimization)\n",
    "manual_model = xgb.XGBRegressor(\n",
    "    n_estimators=150,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "manual_model.fit(X_train_np, y_train_np)\n",
    "manual_predictions = manual_model.predict(X_val_np)\n",
    "manual_predictions = np.round(manual_predictions).astype(int)\n",
    "\n",
    "# Calculate metrics\n",
    "manual_mse = mean_squared_error(y_val_np, manual_predictions)\n",
    "manual_rmse = np.sqrt(manual_mse)\n",
    "manual_r2 = r2_score(y_val_np, manual_predictions)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FRAMEWORK vs MANUAL APPROACH COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Framework (Optimized) Results:\")\n",
    "print(f\"  RMSE: {eval_result['metrics']['rmse']:.4f}\")\n",
    "print(f\"  R²: {eval_result['metrics']['r2']:.4f}\")\n",
    "print(f\"  Hyperparameter trials: {training_config.n_trials}\")\n",
    "\n",
    "print(f\"\\nManual (Basic) Results:\")\n",
    "print(f\"  RMSE: {manual_rmse:.4f}\")\n",
    "print(f\"  R²: {manual_r2:.4f}\")\n",
    "print(f\"  Hyperparameter trials: 0 (fixed parameters)\")\n",
    "\n",
    "improvement = ((manual_rmse - eval_result['metrics']['rmse']) / manual_rmse) * 100\n",
    "print(f\"\\nFramework Improvement: {improvement:.2f}% better RMSE\")\n",
    "\n",
    "print(\"\\nFramework Advantages:\")\n",
    "print(\"+ Systematic hyperparameter optimization\")\n",
    "print(\"+ Automatic model storage and metadata tracking\")\n",
    "print(\"+ Comprehensive evaluation metrics\")\n",
    "print(\"+ Reproducible configuration management\")\n",
    "print(\"+ Easy scaling to multiple models/granularities\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Demonstration of Multi-Granularity Capability\n",
    "\n",
    "Show how the framework handles different granularity levels (SKU vs Product vs Store)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demonstrating multi-granularity capability...\n",
      "\n",
      "Sample Product ID: 79646\n",
      "\n",
      "Granularity Comparison:\n",
      "SKU-level data shape: (870, 53)\n",
      "Product-level data shape: (877, 46)\n",
      "\n",
      "Aggregation Effect:\n",
      "SKU-level: Individual product-store combinations\n",
      "Product-level: Sales summed across stores, prices averaged\n",
      "\n",
      "Sample Store ID: 1335\n",
      "Store-level data shape: (1941, 46)\n",
      "\n",
      "Store-level: All products within store aggregated by date\n",
      "\n",
      "==================================================\n",
      "MULTI-GRANULARITY SUMMARY\n",
      "==================================================\n",
      "✓ SKU Level: 870 observations (finest granularity)\n",
      "✓ Product Level: 877 observations (aggregated across stores)\n",
      "✓ Store Level: 1941 observations (aggregated across products)\n",
      "\n",
      "The framework can train models at any of these granularity levels!\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate product-level modeling (aggregating across stores)\n",
    "print(\"Demonstrating multi-granularity capability...\")\n",
    "\n",
    "# Get a product ID from our SKU\n",
    "sample_product_id = sku_features.select(\"productID\").unique().item()\n",
    "print(f\"\\nSample Product ID: {sample_product_id}\")\n",
    "\n",
    "# Get product-level data (aggregated across stores)\n",
    "product_features, product_target = data_loader.get_data_for_granularity(\n",
    "    granularity=GranularityLevel.PRODUCT,\n",
    "    entity_ids={\"productID\": sample_product_id},\n",
    "    collect=True\n",
    ")\n",
    "\n",
    "print(f\"\\nGranularity Comparison:\")\n",
    "print(f\"SKU-level data shape: {sku_features.shape}\")\n",
    "print(f\"Product-level data shape: {product_features.shape}\")\n",
    "\n",
    "# Show aggregation effect\n",
    "print(f\"\\nAggregation Effect:\")\n",
    "print(f\"SKU-level: Individual product-store combinations\")\n",
    "print(f\"Product-level: Sales summed across stores, prices averaged\")\n",
    "\n",
    "# Get sample store ID for demonstration\n",
    "sample_store_id = sku_features.select(\"storeID\").unique().item()\n",
    "print(f\"\\nSample Store ID: {sample_store_id}\")\n",
    "\n",
    "store_features, store_target = data_loader.get_data_for_granularity(\n",
    "    granularity=GranularityLevel.STORE,\n",
    "    entity_ids={\"storeID\": sample_store_id},\n",
    "    collect=True\n",
    ")\n",
    "\n",
    "print(f\"Store-level data shape: {store_features.shape}\")\n",
    "print(f\"\\nStore-level: All products within store aggregated by date\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MULTI-GRANULARITY SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"✓ SKU Level: {sku_features.shape[0]} observations (finest granularity)\")\n",
    "print(f\"✓ Product Level: {product_features.shape[0]} observations (aggregated across stores)\")\n",
    "print(f\"✓ Store Level: {store_features.shape[0]} observations (aggregated across products)\")\n",
    "print(\"\\nThe framework can train models at any of these granularity levels!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Complete Pipeline Demonstration\n",
    "\n",
    "Finally, let's show how to use the BenchmarkPipeline for end-to-end workflow automation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demonstrating complete BenchmarkPipeline...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div id=\"GAlGW9\"></div>\n",
       "            <script type=\"text/javascript\" data-lets-plot-script=\"library\">\n",
       "                if(!window.letsPlotCallQueue) {\n",
       "                    window.letsPlotCallQueue = [];\n",
       "                }; \n",
       "                window.letsPlotCall = function(f) {\n",
       "                    window.letsPlotCallQueue.push(f);\n",
       "                };\n",
       "                (function() {\n",
       "                    var script = document.createElement(\"script\");\n",
       "                    script.type = \"text/javascript\";\n",
       "                    script.src = \"https://cdn.jsdelivr.net/gh/JetBrains/lets-plot@v4.6.2/js-package/distr/lets-plot.min.js\";\n",
       "                    script.onload = function() {\n",
       "                        window.letsPlotCall = function(f) {f();};\n",
       "                        window.letsPlotCallQueue.forEach(function(f) {f();});\n",
       "                        window.letsPlotCallQueue = [];\n",
       "                        \n",
       "                    };\n",
       "                    script.onerror = function(event) {\n",
       "                        window.letsPlotCall = function(f) {};    // noop\n",
       "                        window.letsPlotCallQueue = [];\n",
       "                        var div = document.createElement(\"div\");\n",
       "                        div.style.color = 'darkred';\n",
       "                        div.textContent = 'Error loading Lets-Plot JS';\n",
       "                        document.getElementById(\"GAlGW9\").appendChild(div);\n",
       "                    };\n",
       "                    var e = document.getElementById(\"GAlGW9\");\n",
       "                    e.appendChild(script);\n",
       "                })()\n",
       "            </script>\n",
       "            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-22 17:58:51,732 - INFO - Loading and preparing M5 dataset...\n",
      "2025-07-22 17:58:51,732 - INFO - Loading M5 dataset...\n",
      "2025-07-22 17:59:13,697 - INFO - Data loading completed\n",
      "2025-07-22 17:59:13,699 - INFO - Data loading completed\n",
      "2025-07-22 17:59:13,700 - INFO - Running experiment: pipeline_demo\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running complete pipeline for SKU 278314...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-22 17:59:15,337 - INFO - Creating features for sku level\n",
      "2025-07-22 17:59:15,346 - INFO - Created 137 features\n",
      "2025-07-22 17:59:15,364 - INFO - Prepared data: 863 samples, 137 features\n",
      "2025-07-22 17:59:15,364 - INFO - Dataset prepared: 863 samples, 137 features\n",
      "2025-07-22 17:59:15,370 - INFO - Created temporal split: 690 train, 173 validation\n",
      "2025-07-22 17:59:15,370 - INFO - Split date: 2015-12-02\n",
      "2025-07-22 17:59:15,377 - INFO - Training xgboost model for sku level\n",
      "[I 2025-07-22 17:59:15,415] A new study created in memory with name: no-name-dfd7b215-3fbe-4f33-8722-2fc54a854851\n",
      "[I 2025-07-22 17:59:15,505] Trial 0 finished with value: 0.0 and parameters: {'n_estimators': 228, 'max_depth': 5, 'learning_rate': 0.46959348318250904, 'subsample': 0.9302433491347639, 'colsample_bytree': 0.8285273086530917, 'reg_alpha': 4.487923873513936, 'reg_lambda': 7.87127055692026}. Best is trial 0 with value: 0.0.\n",
      "[I 2025-07-22 17:59:15,569] Trial 1 finished with value: 0.0 and parameters: {'n_estimators': 144, 'max_depth': 10, 'learning_rate': 0.24353225177018978, 'subsample': 0.9634105085431155, 'colsample_bytree': 0.7067682926041247, 'reg_alpha': 2.8138196762462564, 'reg_lambda': 9.876317739127247}. Best is trial 0 with value: 0.0.\n",
      "[I 2025-07-22 17:59:15,633] Trial 2 finished with value: 0.0 and parameters: {'n_estimators': 103, 'max_depth': 15, 'learning_rate': 0.1392917235897033, 'subsample': 0.792988557854331, 'colsample_bytree': 0.9324797171565752, 'reg_alpha': 2.6634711099989614, 'reg_lambda': 6.865286420353725}. Best is trial 0 with value: 0.0.\n",
      "[I 2025-07-22 17:59:15,709] Trial 3 finished with value: 0.0 and parameters: {'n_estimators': 239, 'max_depth': 17, 'learning_rate': 0.22353570562476732, 'subsample': 0.8950648816037152, 'colsample_bytree': 0.8933770981266518, 'reg_alpha': 5.914643320367571, 'reg_lambda': 7.304374646169642}. Best is trial 0 with value: 0.0.\n",
      "[I 2025-07-22 17:59:15,773] Trial 4 finished with value: 0.0 and parameters: {'n_estimators': 152, 'max_depth': 6, 'learning_rate': 0.0784327290200375, 'subsample': 0.8545922831031278, 'colsample_bytree': 0.9738604491057806, 'reg_alpha': 3.238190507977694, 'reg_lambda': 1.3983227612451312}. Best is trial 0 with value: 0.0.\n",
      "2025-07-22 17:59:15,773 - INFO - Best parameters: {'n_estimators': 228, 'max_depth': 5, 'learning_rate': 0.46959348318250904, 'subsample': 0.9302433491347639, 'colsample_bytree': 0.8285273086530917, 'reg_alpha': 4.487923873513936, 'reg_lambda': 7.87127055692026}\n",
      "2025-07-22 17:59:15,774 - INFO - Best validation MSE: 0.0000\n",
      "2025-07-22 17:59:15,854 - INFO - Model training completed. Validation MSE: 0.0000\n",
      "2025-07-22 17:59:15,857 - INFO - Experiment pipeline_demo completed. Model ID: sku_278314_xgboost\n",
      "2025-07-22 17:59:15,858 - INFO - Evaluating all models in registry\n",
      "2025-07-22 17:59:15,858 - INFO - Evaluating all models at sku level\n",
      "2025-07-22 17:59:15,858 - INFO - Comparing 1 models\n",
      "2025-07-22 17:59:15,858 - INFO - Evaluating model: sku_278314_xgboost\n",
      "2025-07-22 17:59:15,859 - WARNING - Use evaluate_model_with_data() method instead for notebook usage\n",
      "2025-07-22 17:59:15,859 - INFO - Evaluating all models at product level\n",
      "2025-07-22 17:59:15,859 - WARNING - No models found for granularity product\n",
      "2025-07-22 17:59:15,860 - INFO - Evaluating all models at store level\n",
      "2025-07-22 17:59:15,860 - WARNING - No models found for granularity store\n",
      "2025-07-22 17:59:15,860 - INFO - Comparing 1 models\n",
      "2025-07-22 17:59:15,861 - INFO - Evaluating model: sku_278314_xgboost\n",
      "2025-07-22 17:59:15,861 - WARNING - Use evaluate_model_with_data() method instead for notebook usage\n",
      "2025-07-22 17:59:15,867 - INFO - Report saved to pipeline_demo_results/evaluation_results/sku_evaluation_report.md\n",
      "2025-07-22 17:59:15,868 - INFO - Evaluation results saved to pipeline_demo_results/evaluation_results\n",
      "2025-07-22 17:59:15,869 - INFO - Experiment log saved to pipeline_demo_results/experiment_log.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PIPELINE DEMONSTRATION COMPLETE\n",
      "============================================================\n",
      "✓ Model trained and saved: sku_278314_xgboost\n",
      "✓ Evaluation results generated\n",
      "✓ Experiment log saved to: pipeline_demo_results/experiment_log.json\n",
      "✓ Model registry at: pipeline_demo_results/models/\n",
      "\n",
      "Experiment Log Summary:\n",
      "  - pipeline_demo: 863 samples, 137 features\n",
      "    Performance: RMSE 0.0\n"
     ]
    }
   ],
   "source": [
    "# Initialize the complete pipeline (highest-level interface)\n",
    "print(\"Demonstrating complete BenchmarkPipeline...\")\n",
    "\n",
    "# Use faster configuration for demo\n",
    "demo_training_config = TrainingConfig(\n",
    "    validation_split=0.2,\n",
    "    n_trials=5,  # Very fast for demo\n",
    "    model_type=\"xgboost\"\n",
    ")\n",
    "\n",
    "pipeline = BenchmarkPipeline(\n",
    "    data_config=data_config,\n",
    "    training_config=demo_training_config,\n",
    "    output_dir=Path(\"pipeline_demo_results\")\n",
    ")\n",
    "\n",
    "# Load data once\n",
    "pipeline.load_and_prepare_data()\n",
    "\n",
    "# Run a single model experiment with the pipeline\n",
    "print(f\"\\nRunning complete pipeline for SKU {demo_sku_id}...\")\n",
    "\n",
    "pipeline_model = pipeline.run_single_model_experiment(\n",
    "    granularity=GranularityLevel.SKU,\n",
    "    entity_ids={\"skuID\": demo_sku_id},\n",
    "    experiment_name=\"pipeline_demo\"\n",
    ")\n",
    "\n",
    "# Evaluate using pipeline\n",
    "pipeline_results = pipeline.evaluate_all_models()\n",
    "\n",
    "# Save experiment log\n",
    "pipeline.save_experiment_log()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PIPELINE DEMONSTRATION COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"✓ Model trained and saved: {pipeline_model.get_identifier()}\")\n",
    "print(f\"✓ Evaluation results generated\")\n",
    "print(f\"✓ Experiment log saved to: pipeline_demo_results/experiment_log.json\")\n",
    "print(f\"✓ Model registry at: pipeline_demo_results/models/\")\n",
    "\n",
    "# Show experiment log preview\n",
    "if pipeline.experiment_log:\n",
    "    print(f\"\\nExperiment Log Summary:\")\n",
    "    for exp in pipeline.experiment_log:\n",
    "        print(f\"  - {exp['experiment_name']}: {exp['n_samples']} samples, {exp['n_features']} features\")\n",
    "        print(f\"    Performance: RMSE {exp['performance'].get('rmse', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Loading and Using Previously Saved Models\n",
    "\n",
    "This section demonstrates how to load multiple models that were trained and saved in previous sessions, evaluate them, and generate predictions. This is crucial for production scenarios where you have a collection of trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 SIMULATING MULTIPLE TRAINED MODELS\n",
      "============================================================\n",
      "\n",
      "📊 Training Model 1/3: sku_model_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-22 17:59:33,194 - INFO - Creating features for sku level\n",
      "2025-07-22 17:59:33,198 - INFO - Created 137 features\n",
      "2025-07-22 17:59:33,219 - INFO - Prepared data: 863 samples, 137 features\n",
      "2025-07-22 17:59:33,221 - INFO - Created temporal split: 690 train, 173 validation\n",
      "2025-07-22 17:59:33,221 - INFO - Split date: 2015-12-02\n",
      "2025-07-22 17:59:33,229 - INFO - Training xgboost model for sku level\n",
      "[I 2025-07-22 17:59:33,232] A new study created in memory with name: no-name-4935667b-605d-4a57-8edf-f02fdfd90440\n",
      "[I 2025-07-22 17:59:33,308] Trial 0 finished with value: 0.0 and parameters: {'n_estimators': 164, 'max_depth': 7, 'learning_rate': 0.15943194225533244, 'subsample': 0.9775428134842084, 'colsample_bytree': 0.8327594969573855, 'reg_alpha': 5.62729631820901, 'reg_lambda': 9.055628650527122}. Best is trial 0 with value: 0.0.\n",
      "[I 2025-07-22 17:59:33,343] Trial 1 finished with value: 0.005780346820809248 and parameters: {'n_estimators': 64, 'max_depth': 19, 'learning_rate': 0.14810363630888368, 'subsample': 0.9654576191048728, 'colsample_bytree': 0.7860346079490916, 'reg_alpha': 7.091622474941419, 'reg_lambda': 6.942151234325728}. Best is trial 0 with value: 0.0.\n",
      "[I 2025-07-22 17:59:33,391] Trial 2 finished with value: 0.005780346820809248 and parameters: {'n_estimators': 157, 'max_depth': 6, 'learning_rate': 0.3505197234474381, 'subsample': 0.7811642088455086, 'colsample_bytree': 0.9369434353433537, 'reg_alpha': 9.642942061543726, 'reg_lambda': 9.896666975949877}. Best is trial 0 with value: 0.0.\n",
      "2025-07-22 17:59:33,391 - INFO - Best parameters: {'n_estimators': 164, 'max_depth': 7, 'learning_rate': 0.15943194225533244, 'subsample': 0.9775428134842084, 'colsample_bytree': 0.8327594969573855, 'reg_alpha': 5.62729631820901, 'reg_lambda': 9.055628650527122}\n",
      "2025-07-22 17:59:33,391 - INFO - Best validation MSE: 0.0000\n",
      "2025-07-22 17:59:33,465 - INFO - Model training completed. Validation MSE: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Model saved: sku_278314_xgboost\n",
      "   📊 Performance: RMSE 0.0000\n",
      "\n",
      "📊 Training Model 2/3: product_model_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-22 17:59:34,152 - INFO - Creating features for product level\n",
      "2025-07-22 17:59:34,154 - INFO - Created 123 features\n",
      "2025-07-22 17:59:34,157 - INFO - Prepared data: 877 samples, 123 features\n",
      "2025-07-22 17:59:34,158 - INFO - Created temporal split: 701 train, 176 validation\n",
      "2025-07-22 17:59:34,158 - INFO - Split date: 2015-11-29\n",
      "2025-07-22 17:59:34,160 - INFO - Training xgboost model for product level\n",
      "[I 2025-07-22 17:59:34,162] A new study created in memory with name: no-name-2c70ba2c-aaba-413a-b092-bd464a3a6178\n",
      "[I 2025-07-22 17:59:34,338] Trial 0 finished with value: 0.022727272727272728 and parameters: {'n_estimators': 221, 'max_depth': 16, 'learning_rate': 0.05640946756269847, 'subsample': 0.929317375857593, 'colsample_bytree': 0.8489468175526627, 'reg_alpha': 6.7322683499139755, 'reg_lambda': 9.001704232487171}. Best is trial 0 with value: 0.022727272727272728.\n",
      "[I 2025-07-22 17:59:34,399] Trial 1 finished with value: 0.022727272727272728 and parameters: {'n_estimators': 131, 'max_depth': 8, 'learning_rate': 0.30680384160860363, 'subsample': 0.8744717993838054, 'colsample_bytree': 0.9629978263720922, 'reg_alpha': 5.381868572052316, 'reg_lambda': 9.097450826973049}. Best is trial 0 with value: 0.022727272727272728.\n",
      "[I 2025-07-22 17:59:35,014] Trial 2 finished with value: 0.022727272727272728 and parameters: {'n_estimators': 226, 'max_depth': 19, 'learning_rate': 0.06490651891468159, 'subsample': 0.7292005868485338, 'colsample_bytree': 0.9526144397833697, 'reg_alpha': 1.3869218788766358, 'reg_lambda': 8.20123807419989}. Best is trial 0 with value: 0.022727272727272728.\n",
      "2025-07-22 17:59:35,015 - INFO - Best parameters: {'n_estimators': 221, 'max_depth': 16, 'learning_rate': 0.05640946756269847, 'subsample': 0.929317375857593, 'colsample_bytree': 0.8489468175526627, 'reg_alpha': 6.7322683499139755, 'reg_lambda': 9.001704232487171}\n",
      "2025-07-22 17:59:35,015 - INFO - Best validation MSE: 0.0227\n",
      "2025-07-22 17:59:35,159 - INFO - Model training completed. Validation MSE: 0.0227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Model saved: product_79646_xgboost\n",
      "   📊 Performance: RMSE 0.1508\n",
      "\n",
      "📊 Training Model 3/3: store_model_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-22 17:59:37,042 - INFO - Creating features for store level\n",
      "2025-07-22 17:59:37,049 - INFO - Created 139 features\n",
      "2025-07-22 17:59:37,052 - INFO - Prepared data: 1934 samples, 139 features\n",
      "2025-07-22 17:59:37,053 - INFO - Created temporal split: 1547 train, 387 validation\n",
      "2025-07-22 17:59:37,054 - INFO - Split date: 2015-05-02\n",
      "2025-07-22 17:59:37,057 - INFO - Training xgboost model for store level\n",
      "[I 2025-07-22 17:59:37,060] A new study created in memory with name: no-name-58caf37a-d53e-4a94-b92b-0945252eca62\n",
      "[I 2025-07-22 17:59:37,371] Trial 0 finished with value: 1.669250645994832 and parameters: {'n_estimators': 222, 'max_depth': 14, 'learning_rate': 0.4569438268437564, 'subsample': 0.9201649680008896, 'colsample_bytree': 0.851748169170462, 'reg_alpha': 7.657321810772463, 'reg_lambda': 2.7568415222778024}. Best is trial 0 with value: 1.669250645994832.\n",
      "[I 2025-07-22 17:59:37,702] Trial 1 finished with value: 1.5271317829457365 and parameters: {'n_estimators': 158, 'max_depth': 10, 'learning_rate': 0.10781448010412865, 'subsample': 0.9053975955986382, 'colsample_bytree': 0.8922275418635621, 'reg_alpha': 3.952119054866979, 'reg_lambda': 6.6778442718417725}. Best is trial 1 with value: 1.5271317829457365.\n",
      "[I 2025-07-22 17:59:38,116] Trial 2 finished with value: 1.6175710594315245 and parameters: {'n_estimators': 178, 'max_depth': 17, 'learning_rate': 0.2822430242229835, 'subsample': 0.8710843085336112, 'colsample_bytree': 0.9160529625545748, 'reg_alpha': 7.032787139863571, 'reg_lambda': 9.725395451588748}. Best is trial 1 with value: 1.5271317829457365.\n",
      "2025-07-22 17:59:38,116 - INFO - Best parameters: {'n_estimators': 158, 'max_depth': 10, 'learning_rate': 0.10781448010412865, 'subsample': 0.9053975955986382, 'colsample_bytree': 0.8922275418635621, 'reg_alpha': 3.952119054866979, 'reg_lambda': 6.6778442718417725}\n",
      "2025-07-22 17:59:38,117 - INFO - Best validation MSE: 1.5271\n",
      "2025-07-22 17:59:38,450 - INFO - Model training completed. Validation MSE: 1.5762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Model saved: store_1335_xgboost\n",
      "   📊 Performance: RMSE 1.2555\n",
      "\n",
      "🎉 Successfully trained and saved 3 models!\n",
      "📁 Models saved in: saved_models_demo\n"
     ]
    }
   ],
   "source": [
    "# ==================================================================================\n",
    "# SCENARIO: You have multiple models trained and saved from previous sessions\n",
    "# Let's simulate this by training a few more models first, then show how to load them\n",
    "# ==================================================================================\n",
    "\n",
    "print(\"🔄 SIMULATING MULTIPLE TRAINED MODELS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create a separate registry for this demonstration\n",
    "saved_models_registry = ModelRegistry(storage_path=Path(\"saved_models_demo\"))\n",
    "\n",
    "# FIXED: Define the missing variables here or get them from existing data\n",
    "# Get sample product and store IDs from the SKU data we already have\n",
    "sample_product_id = sku_features.select(\"productID\").unique().item()\n",
    "sample_store_id = sku_features.select(\"storeID\").unique().item()\n",
    "\n",
    "# Train a few different models to simulate having multiple saved models\n",
    "demo_entities = [\n",
    "    {\"granularity\": GranularityLevel.SKU, \"entity_ids\": {\"skuID\": demo_sku_id}, \"name\": \"sku_model_1\"},\n",
    "    {\"granularity\": GranularityLevel.PRODUCT, \"entity_ids\": {\"productID\": sample_product_id}, \"name\": \"product_model_1\"},\n",
    "    {\"granularity\": GranularityLevel.STORE, \"entity_ids\": {\"storeID\": sample_store_id}, \"name\": \"store_model_1\"}\n",
    "]\n",
    "\n",
    "# Quick training configuration for demo\n",
    "quick_training_config = TrainingConfig(\n",
    "    validation_split=0.2,\n",
    "    n_trials=3,  # Very fast for demo\n",
    "    model_type=\"xgboost\",\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "quick_trainer = ModelTrainer(quick_training_config)\n",
    "trained_model_ids = []\n",
    "\n",
    "for i, entity_info in enumerate(demo_entities, 1):\n",
    "    print(f\"\\n📊 Training Model {i}/3: {entity_info['name']}\")\n",
    "    \n",
    "    # Get data for this granularity\n",
    "    entity_features, entity_target = data_loader.get_data_for_granularity(\n",
    "        entity_info[\"granularity\"],\n",
    "        entity_info[\"entity_ids\"],\n",
    "        collect=True\n",
    "    )\n",
    "    \n",
    "    # Engineer features\n",
    "    entity_engineered, entity_feature_cols = feature_engineer.create_features(\n",
    "        entity_features, entity_target, \n",
    "        entity_info[\"granularity\"], entity_info[\"entity_ids\"]\n",
    "    )\n",
    "    \n",
    "    # Prepare model data\n",
    "    entity_X, entity_y = feature_engineer.prepare_model_data(\n",
    "        entity_engineered, entity_feature_cols, data_config.target_column\n",
    "    )\n",
    "    \n",
    "    # Create temporal split\n",
    "    train_ids, val_ids, _ = data_loader.create_temporal_split(entity_X, 0.2)\n",
    "    \n",
    "    entity_X_train = entity_X.filter(pl.col(\"bdID\").is_in(train_ids))\n",
    "    entity_y_train = entity_y.filter(pl.col(\"bdID\").is_in(train_ids))\n",
    "    entity_X_val = entity_X.filter(pl.col(\"bdID\").is_in(val_ids))\n",
    "    entity_y_val = entity_y.filter(pl.col(\"bdID\").is_in(val_ids))\n",
    "    \n",
    "    # Train model\n",
    "    model = quick_trainer.train_model(\n",
    "        entity_X_train, entity_y_train, entity_X_val, entity_y_val,\n",
    "        entity_feature_cols, data_config.target_column,\n",
    "        entity_info[\"granularity\"], entity_info[\"entity_ids\"]\n",
    "    )\n",
    "    \n",
    "    # Save model\n",
    "    model_id = saved_models_registry.register_model(model)\n",
    "    saved_models_registry.save_model(model_id)\n",
    "    trained_model_ids.append(model_id)\n",
    "    \n",
    "    print(f\"   ✅ Model saved: {model_id}\")\n",
    "    print(f\"   📊 Performance: RMSE {model.metadata.performance_metrics.get('rmse', 0):.4f}\")\n",
    "\n",
    "print(f\"\\n🎉 Successfully trained and saved {len(trained_model_ids)} models!\")\n",
    "print(f\"📁 Models saved in: {saved_models_registry.storage_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 17.1 Loading Previously Saved Models\n",
    "\n",
    "Now let's demonstrate how to load models that were saved in previous sessions. This is what you would do when you restart your Python session and want to work with models you trained earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 LOADING PREVIOUSLY SAVED MODELS FROM DISK\n",
      "============================================================\n",
      "\n",
      "📋 Discovering saved models...\n",
      "Found 0 saved models:\n",
      "\n",
      "📊 Models by granularity:\n",
      "  SKU level: 0 models\n",
      "  PRODUCT level: 0 models\n",
      "  STORE level: 0 models\n",
      "\n",
      "📥 Loading models from disk...\n",
      "\n",
      "🎉 Successfully loaded 0 models from disk!\n"
     ]
    }
   ],
   "source": [
    "# ==================================================================================\n",
    "# LOADING PREVIOUSLY SAVED MODELS - This is what you'd do in a new Python session\n",
    "# ==================================================================================\n",
    "\n",
    "print(\"🔍 LOADING PREVIOUSLY SAVED MODELS FROM DISK\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# STEP 1: Initialize a fresh model registry (as if in a new session)\n",
    "# In a real scenario, you would just point to your existing model directory\n",
    "production_registry = ModelRegistry(storage_path=Path(\"saved_models_demo\"))\n",
    "\n",
    "# STEP 2: Discover what models are available\n",
    "print(\"\\n📋 Discovering saved models...\")\n",
    "all_saved_models = production_registry.list_models()\n",
    "print(f\"Found {len(all_saved_models)} saved models:\")\n",
    "\n",
    "for i, model_id in enumerate(all_saved_models, 1):\n",
    "    print(f\"  {i}. {model_id}\")\n",
    "\n",
    "# STEP 3: Get models by granularity (useful for organizing your model collection)\n",
    "print(\"\\n📊 Models by granularity:\")\n",
    "for granularity in GranularityLevel:\n",
    "    granularity_models = production_registry.list_models(granularity)\n",
    "    print(f\"  {granularity.value.upper()} level: {len(granularity_models)} models\")\n",
    "    for model_id in granularity_models:\n",
    "        print(f\"    - {model_id}\")\n",
    "\n",
    "# STEP 4: Load specific models\n",
    "print(f\"\\n📥 Loading models from disk...\")\n",
    "loaded_models = {}\n",
    "\n",
    "for model_id in all_saved_models:\n",
    "    print(f\"\\n   Loading: {model_id}\")\n",
    "    \n",
    "    # Load the model (this reads from disk)\n",
    "    model = production_registry.load_model(model_id)\n",
    "    loaded_models[model_id] = model\n",
    "    \n",
    "    # Display key information about the loaded model\n",
    "    print(f\"   ✅ Model Type: {model.metadata.model_type}\")\n",
    "    print(f\"   📊 Granularity: {model.metadata.granularity.value}\")\n",
    "    print(f\"   🎯 Entity: {model.metadata.entity_ids}\")\n",
    "    print(f\"   📈 Features: {len(model.metadata.feature_columns)}\")\n",
    "    print(f\"   🔧 Best Params: {len(model.metadata.hyperparameters)} hyperparameters\")\n",
    "    print(f\"   📉 RMSE: {model.metadata.performance_metrics.get('rmse', 'N/A')}\")\n",
    "    print(f\"   🗓️  Training Range: {model.metadata.training_date_range}\")\n",
    "\n",
    "print(f\"\\n🎉 Successfully loaded {len(loaded_models)} models from disk!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 17.2 Generating Predictions from Loaded Models\n",
    "\n",
    "Now let's use the loaded models to generate predictions on new data. This shows how to use saved models for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔮 GENERATING PREDICTIONS FROM LOADED MODELS\n",
      "============================================================\n",
      "⚠️ loaded_models not found. Loading models from registry...\n",
      "\n",
      "Generating predictions from 0 loaded models...\n",
      "\n",
      "\n",
      "🔧 MANUAL PREDICTION EXAMPLE (for custom data)\n",
      "----------------------------------------\n",
      "❌ No models available for manual prediction example\n"
     ]
    }
   ],
   "source": [
    "# ==================================================================================\n",
    "# GENERATING PREDICTIONS FROM LOADED MODELS\n",
    "# ==================================================================================\n",
    "\n",
    "print(\"🔮 GENERATING PREDICTIONS FROM LOADED MODELS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# FIXED: Add safety check and initialization if variables don't exist\n",
    "if 'production_registry' not in locals():\n",
    "    print(\"⚠️ production_registry not found. Initializing...\")\n",
    "    production_registry = ModelRegistry(storage_path=Path(\"saved_models_demo\"))\n",
    "\n",
    "if 'loaded_models' not in locals() or not loaded_models:\n",
    "    print(\"⚠️ loaded_models not found. Loading models from registry...\")\n",
    "    all_saved_models = production_registry.list_models()\n",
    "    loaded_models = {}\n",
    "    for model_id in all_saved_models:\n",
    "        loaded_models[model_id] = production_registry.load_model(model_id)\n",
    "\n",
    "# Initialize evaluator with the production registry\n",
    "production_evaluator = ModelEvaluator(data_loader, production_registry)\n",
    "\n",
    "# Dictionary to store all predictions\n",
    "model_predictions = {}\n",
    "model_evaluations = {}\n",
    "\n",
    "print(f\"\\nGenerating predictions from {len(loaded_models)} loaded models...\\n\")\n",
    "\n",
    "for model_id, model in loaded_models.items():\n",
    "    print(f\"🔮 Generating predictions for: {model_id}\")\n",
    "    \n",
    "    # METHOD 1: Use the evaluator to get predictions and evaluation metrics\n",
    "    evaluation_result = production_evaluator.evaluate_model(model)\n",
    "    \n",
    "    if \"error\" not in evaluation_result:\n",
    "        model_evaluations[model_id] = evaluation_result\n",
    "        model_predictions[model_id] = {\n",
    "            \"predictions\": evaluation_result[\"predictions\"],\n",
    "            \"actuals\": evaluation_result[\"actuals\"],\n",
    "            \"metrics\": evaluation_result[\"metrics\"],\n",
    "            \"granularity\": evaluation_result[\"granularity\"],\n",
    "            \"entity_ids\": evaluation_result[\"entity_ids\"]\n",
    "        }\n",
    "        \n",
    "        print(f\"   ✅ Generated {len(evaluation_result['predictions'])} predictions\")\n",
    "        print(f\"   📊 RMSE: {evaluation_result['metrics']['rmse']:.4f}\")\n",
    "        print(f\"   📈 R²: {evaluation_result['metrics']['r2']:.4f}\")\n",
    "        print(f\"   🎯 MAPE: {evaluation_result['metrics']['mape']:.2f}%\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"   ❌ Error: {evaluation_result['error']}\")\n",
    "    \n",
    "    print(\"\")\n",
    "\n",
    "# METHOD 2: Manual prediction generation (for custom scenarios)\n",
    "print(f\"\\n🔧 MANUAL PREDICTION EXAMPLE (for custom data)\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Let's demonstrate manual prediction with one of our models\n",
    "if loaded_models:\n",
    "    example_model_id = list(loaded_models.keys())[0]  # Take first model\n",
    "    example_model = loaded_models[example_model_id]\n",
    "\n",
    "    print(f\"Using model: {example_model_id}\")\n",
    "    print(f\"Model granularity: {example_model.metadata.granularity.value}\")\n",
    "    print(f\"Model entity: {example_model.metadata.entity_ids}\")\n",
    "\n",
    "    # Get fresh data for this model's entity (simulating new data for prediction)\n",
    "    fresh_features, fresh_target = data_loader.get_data_for_granularity(\n",
    "        example_model.metadata.granularity,\n",
    "        example_model.metadata.entity_ids,\n",
    "        collect=True\n",
    "    )\n",
    "\n",
    "    # Engineer features using the same feature columns as the trained model\n",
    "    fresh_engineered, _ = feature_engineer.create_features(\n",
    "        fresh_features, fresh_target,\n",
    "        example_model.metadata.granularity,\n",
    "        example_model.metadata.entity_ids\n",
    "    )\n",
    "\n",
    "    # Prepare data for prediction - use ONLY the features the model was trained on\n",
    "    fresh_X, fresh_y = feature_engineer.prepare_model_data(\n",
    "        fresh_engineered, \n",
    "        example_model.metadata.feature_columns,  # IMPORTANT: Use model's original features\n",
    "        example_model.metadata.target_column\n",
    "    )\n",
    "\n",
    "    # Take a sample for manual prediction (e.g., last 100 observations)\n",
    "    sample_size = min(100, len(fresh_X))\n",
    "    X_sample = fresh_X.tail(sample_size).select(example_model.metadata.feature_columns).to_numpy()\n",
    "    y_sample = fresh_y.tail(sample_size).select(example_model.metadata.target_column).to_numpy().flatten()\n",
    "\n",
    "    print(f\"\\nMaking predictions on {sample_size} fresh samples...\")\n",
    "\n",
    "    # Generate predictions using the loaded model\n",
    "    manual_predictions = example_model.model.predict(X_sample)\n",
    "    manual_predictions = np.round(manual_predictions).astype(int)\n",
    "\n",
    "    # Calculate metrics\n",
    "    manual_rmse = np.sqrt(mean_squared_error(y_sample, manual_predictions))\n",
    "    manual_r2 = r2_score(y_sample, manual_predictions)\n",
    "\n",
    "    print(f\"Manual prediction results:\")\n",
    "    print(f\"   📊 Sample size: {sample_size}\")\n",
    "    print(f\"   📉 RMSE: {manual_rmse:.4f}\")\n",
    "    print(f\"   📈 R²: {manual_r2:.4f}\")\n",
    "    print(f\"   🎯 Mean prediction: {np.mean(manual_predictions):.2f}\")\n",
    "    print(f\"   📏 Prediction range: [{np.min(manual_predictions)}, {np.max(manual_predictions)}]\")\n",
    "else:\n",
    "    print(\"❌ No models available for manual prediction example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 17.3 Comparing Multiple Loaded Models\n",
    "\n",
    "Let's compare the performance of all our loaded models to see which ones perform best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "# COMPARING MULTIPLE LOADED MODELS\n",
    "# ==================================================================================\n",
    "\n",
    "print(\"📊 COMPARING PERFORMANCE OF ALL LOADED MODELS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Use the evaluator to compare all models\n",
    "if len(all_saved_models) > 1:\n",
    "    print(f\"Comparing {len(all_saved_models)} models...\")\n",
    "    \n",
    "    # Compare all models using the evaluator\n",
    "    comparison_results = production_evaluator.compare_models(all_saved_models)\n",
    "    \n",
    "    if \"error\" not in comparison_results:\n",
    "        print(f\"\\n📈 MODEL PERFORMANCE COMPARISON\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Display metrics comparison in a table format\n",
    "        if \"metrics_comparison\" in comparison_results:\n",
    "            metrics_df = pd.DataFrame(comparison_results[\"metrics_comparison\"]).T\n",
    "            print(\"\\nPerformance Metrics Table:\")\n",
    "            print(metrics_df.round(4))\n",
    "        \n",
    "        # Show rankings\n",
    "        if \"rankings\" in comparison_results:\n",
    "            print(f\"\\n🏆 MODEL RANKINGS\")\n",
    "            print(\"-\" * 25)\n",
    "            \n",
    "            key_metrics = [\"rmse\", \"r2\", \"mape\"]\n",
    "            for metric in key_metrics:\n",
    "                if metric in comparison_results[\"rankings\"]:\n",
    "                    print(f\"\\n🎯 Best models by {metric.upper()}:\")\n",
    "                    ranking = comparison_results[\"rankings\"][metric]\n",
    "                    for i, (model_id, value) in enumerate(ranking[:3], 1):  # Top 3\n",
    "                        granularity = loaded_models[model_id].metadata.granularity.value\n",
    "                        entity = loaded_models[model_id].metadata.entity_ids\n",
    "                        print(f\"   {i}. {granularity.upper()} model ({entity}): {value:.4f}\")\n",
    "        \n",
    "        # Generate comparison report\n",
    "        report_path = Path(\"saved_models_demo/model_comparison_report.md\")\n",
    "        comparison_report = production_evaluator.generate_evaluation_report(\n",
    "            comparison_results, \n",
    "            output_path=report_path\n",
    "        )\n",
    "        print(f\"\\n📄 Detailed comparison report saved to: {report_path}\")\n",
    "        \n",
    "        # Create comparison visualization if available\n",
    "        if viz_gen.lets_plot_available and \"metrics_comparison\" in comparison_results:\n",
    "            print(f\"\\n📊 Generating comparison visualization...\")\n",
    "            comparison_plot = viz_gen.create_model_comparison_plot(comparison_results, metric=\"rmse\")\n",
    "            if comparison_plot:\n",
    "                comparison_plot.show()\n",
    "                print(\"📊 Model comparison plot displayed above!\")\n",
    "    \n",
    "    else:\n",
    "        print(f\"❌ Comparison failed: {comparison_results['error']}\")\n",
    "\n",
    "else:\n",
    "    print(\"Only one model available - skipping comparison\")\n",
    "\n",
    "# Summary statistics across all predictions\n",
    "print(f\"\\n📋 PREDICTION SUMMARY ACROSS ALL MODELS\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "if model_predictions:\n",
    "    for model_id, pred_data in model_predictions.items():\n",
    "        predictions = pred_data[\"predictions\"]\n",
    "        actuals = pred_data[\"actuals\"]\n",
    "        granularity = pred_data[\"granularity\"]\n",
    "        \n",
    "        print(f\"\\n🔍 {model_id} ({granularity.upper()}):\")\n",
    "        print(f\"   📊 Samples: {len(predictions)}\")\n",
    "        print(f\"   🎯 Mean Prediction: {np.mean(predictions):.2f}\")\n",
    "        print(f\"   📈 Mean Actual: {np.mean(actuals):.2f}\")\n",
    "        print(f\"   📉 RMSE: {pred_data['metrics']['rmse']:.4f}\")\n",
    "        print(f\"   🎪 Prediction Std: {np.std(predictions):.2f}\")\n",
    "        \n",
    "        # Error analysis\n",
    "        errors = actuals - predictions\n",
    "        print(f\"   ⚡ Mean Error (bias): {np.mean(errors):.2f}\")\n",
    "        print(f\"   🎲 Error Std: {np.std(errors):.2f}\")\n",
    "\n",
    "print(f\"\\n🎉 MODEL LOADING AND EVALUATION COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(\"✅ Loaded multiple models from disk\")  \n",
    "print(\"✅ Generated predictions from all models\")\n",
    "print(\"✅ Compared model performance\")\n",
    "print(\"✅ Created evaluation reports\")\n",
    "print(\"✅ Ready for production inference!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 17.4 Production Inference Template\n",
    "\n",
    "Here's a clean template showing how you would typically load and use saved models in a production environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "# PRODUCTION INFERENCE TEMPLATE\n",
    "# This is how you would typically use the framework in production\n",
    "# ==================================================================================\n",
    "\n",
    "def load_and_predict(model_registry_path, target_entity_ids, granularity_level, data_config_obj=None):\n",
    "    \"\"\"\n",
    "    Production-ready function to load models and generate predictions.\n",
    "    \n",
    "    Args:\n",
    "        model_registry_path (Path): Path to your saved models\n",
    "        target_entity_ids (dict): Entity IDs to predict for (e.g., {\"skuID\": 12345})\n",
    "        granularity_level (GranularityLevel): SKU, PRODUCT, or STORE\n",
    "        data_config_obj (DataConfig): Data configuration object\n",
    "    \n",
    "    Returns:\n",
    "        dict: Predictions and metadata\n",
    "    \"\"\"\n",
    "    \n",
    "    # FIXED: Handle missing data config\n",
    "    if data_config_obj is None:\n",
    "        if 'data_config' in globals():\n",
    "            data_config_obj = data_config\n",
    "        else:\n",
    "            raise ValueError(\"data_config_obj must be provided or data_config must exist in global scope\")\n",
    "    \n",
    "    # Initialize components\n",
    "    registry = ModelRegistry(storage_path=model_registry_path)\n",
    "    data_loader_local = DataLoader(data_config_obj)  # Use parameter instead of global\n",
    "    feature_engineer_local = FeatureEngineer(\n",
    "        lag_features=data_config_obj.lag_features,\n",
    "        calendric_features=data_config_obj.calendric_features,\n",
    "        trend_features=data_config_obj.trend_features\n",
    "    )\n",
    "    \n",
    "    # Find models for the target granularity\n",
    "    available_models = registry.list_models(granularity_level)\n",
    "    \n",
    "    # Filter models that match the target entity (simplified matching)\n",
    "    matching_models = []\n",
    "    for model_id in available_models:\n",
    "        model = registry.load_model(model_id)\n",
    "        if model.metadata.entity_ids == target_entity_ids:\n",
    "            matching_models.append(model)\n",
    "    \n",
    "    if not matching_models:\n",
    "        return {\"error\": f\"No models found for {granularity_level.value} with entities {target_entity_ids}\"}\n",
    "    \n",
    "    # Use the first matching model (in production, you might have selection logic)\n",
    "    selected_model = matching_models[0]\n",
    "    \n",
    "    # Get and prepare data\n",
    "    features_df, target_df = data_loader_local.get_data_for_granularity(\n",
    "        granularity_level, target_entity_ids, collect=True\n",
    "    )\n",
    "    \n",
    "    engineered_df, _ = feature_engineer_local.create_features(\n",
    "        features_df, target_df, granularity_level, target_entity_ids\n",
    "    )\n",
    "    \n",
    "    X, y = feature_engineer_local.prepare_model_data(\n",
    "        engineered_df, \n",
    "        selected_model.metadata.feature_columns,  # Use model's original features\n",
    "        selected_model.metadata.target_column\n",
    "    )\n",
    "    \n",
    "    # Generate predictions\n",
    "    X_pred = X.select(selected_model.metadata.feature_columns).to_numpy()\n",
    "    predictions = selected_model.model.predict(X_pred)\n",
    "    predictions = np.round(predictions).astype(int)\n",
    "    \n",
    "    return {\n",
    "        \"model_id\": selected_model.get_identifier(),\n",
    "        \"predictions\": predictions,\n",
    "        \"n_predictions\": len(predictions),\n",
    "        \"model_metadata\": {\n",
    "            \"granularity\": selected_model.metadata.granularity.value,\n",
    "            \"entity_ids\": selected_model.metadata.entity_ids,\n",
    "            \"rmse\": selected_model.metadata.performance_metrics.get('rmse', 'N/A'),\n",
    "            \"hyperparameters\": selected_model.metadata.hyperparameters\n",
    "        }\n",
    "    }\n",
    "\n",
    "# ==================================================================================\n",
    "# EXAMPLE USAGE OF PRODUCTION TEMPLATE\n",
    "# ==================================================================================\n",
    "\n",
    "print(\"🚀 PRODUCTION INFERENCE TEMPLATE EXAMPLE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# FIXED: Add safety checks for required variables\n",
    "if 'demo_sku_id' not in locals():\n",
    "    print(\"⚠️ demo_sku_id not defined. Using first available SKU...\")\n",
    "    if 'unique_entities' in locals() and unique_entities.get('skuIDs'):\n",
    "        demo_sku_id = unique_entities['skuIDs'][0]\n",
    "    else:\n",
    "        print(\"❌ Cannot proceed: No SKU data available\")\n",
    "        demo_sku_id = \"HOBBIES_1_001_CA_1_validation\"  # Fallback example\n",
    "\n",
    "if 'sample_product_id' not in locals():\n",
    "    print(\"⚠️ sample_product_id not defined. Using fallback...\")\n",
    "    sample_product_id = \"HOBBIES_1_001\"  # Fallback example\n",
    "\n",
    "# Ensure data_config exists\n",
    "if 'data_config' not in locals():\n",
    "    print(\"❌ data_config not found. Cannot proceed with production examples.\")\n",
    "else:\n",
    "    # Example 1: SKU-level prediction\n",
    "    print(\"\\n1️⃣ SKU-level prediction:\")\n",
    "    result_sku = load_and_predict(\n",
    "        model_registry_path=Path(\"saved_models_demo\"),\n",
    "        target_entity_ids={\"skuID\": demo_sku_id},\n",
    "        granularity_level=GranularityLevel.SKU,\n",
    "        data_config_obj=data_config  # Pass explicitly\n",
    "    )\n",
    "\n",
    "    if \"error\" not in result_sku:\n",
    "        print(f\"   ✅ Model: {result_sku['model_id']}\")\n",
    "        print(f\"   📊 Predictions generated: {result_sku['n_predictions']}\")\n",
    "        print(f\"   🎯 Mean prediction: {np.mean(result_sku['predictions']):.2f}\")\n",
    "        print(f\"   📈 Model RMSE: {result_sku['model_metadata']['rmse']}\")\n",
    "    else:\n",
    "        print(f\"   ❌ {result_sku['error']}\")\n",
    "\n",
    "    # Example 2: Product-level prediction  \n",
    "    print(\"\\n2️⃣ Product-level prediction:\")\n",
    "    result_product = load_and_predict(\n",
    "        model_registry_path=Path(\"saved_models_demo\"),\n",
    "        target_entity_ids={\"productID\": sample_product_id},\n",
    "        granularity_level=GranularityLevel.PRODUCT,\n",
    "        data_config_obj=data_config\n",
    "    )\n",
    "\n",
    "    if \"error\" not in result_product:\n",
    "        print(f\"   ✅ Model: {result_product['model_id']}\")\n",
    "        print(f\"   📊 Predictions generated: {result_product['n_predictions']}\")\n",
    "        print(f\"   🎯 Mean prediction: {np.mean(result_product['predictions']):.2f}\")\n",
    "        print(f\"   📈 Model RMSE: {result_product['model_metadata']['rmse']}\")\n",
    "    else:\n",
    "        print(f\"   ❌ {result_product['error']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🎯 KEY TAKEAWAYS FOR PRODUCTION USE\")\n",
    "print(\"=\"*60)\n",
    "print(\"✅ 1. Initialize ModelRegistry pointing to your saved models directory\")\n",
    "print(\"✅ 2. Use list_models() to discover available models\")  \n",
    "print(\"✅ 3. Load specific models with load_model(model_id)\")\n",
    "print(\"✅ 4. Generate features using the SAME feature columns as training\")\n",
    "print(\"✅ 5. Use model.model.predict() for inference\")\n",
    "print(\"✅ 6. All model metadata is preserved and accessible\")\n",
    "print(\"✅ 7. Easy to compare multiple models and select the best one\")\n",
    "print(\"✅ 8. Framework handles all the complexity of data preparation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook has demonstrated how the new M5 Benchmarking Framework systematizes and extends the workflow from your original `load_data.ipynb` notebook:\n",
    "\n",
    "### What We've Accomplished:\n",
    "\n",
    "1. **✅ Structured Data Loading**: Replaced manual file loading with configurable `DataLoader` class\n",
    "2. **✅ Systematic Feature Engineering**: Packaged feature creation into reusable `FeatureEngineer` class\n",
    "3. **✅ Optimized Model Training**: Enhanced XGBoost + Optuna training with `ModelTrainer` class\n",
    "4. **✅ Model Persistence**: Added model storage and retrieval with `ModelRegistry` class\n",
    "5. **✅ Comprehensive Evaluation**: Extended evaluation capabilities with `ModelEvaluator` class\n",
    "6. **✅ Automated Visualization**: Maintained lets-plot visualizations in `VisualizationGenerator` class\n",
    "7. **✅ Multi-Granularity Support**: Demonstrated SKU/Product/Store level modeling\n",
    "8. **✅ End-to-End Pipeline**: Showed complete automation with `BenchmarkPipeline` class\n",
    "9. **✅ Production Model Loading**: Comprehensive guide for loading and using saved models\n",
    "\n",
    "### Key Advantages Over Original Notebook:\n",
    "\n",
    "- **🚀 Reproducibility**: Configuration-driven approach ensures consistent results\n",
    "- **📊 Scalability**: Easy to train multiple models across different granularities\n",
    "- **💾 Persistence**: Models and metadata automatically saved for later use\n",
    "- **📈 Comprehensive Metrics**: Extended evaluation beyond basic RMSE/R²\n",
    "- **🔄 Reusability**: Modular components can be mixed and matched\n",
    "- **📝 Documentation**: Automatic report generation and experiment logging\n",
    "- **🏭 Production Ready**: Complete model loading and inference capabilities\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Scale Up**: Use the framework to train models across multiple SKUs/products/stores\n",
    "2. **Custom Models**: Add your custom model types using the extension mechanisms\n",
    "3. **Benchmark Suite**: Run comprehensive benchmarks with `run_full_benchmark_suite()`\n",
    "4. **Production Use**: Deploy models using the registry system for operational forecasting\n",
    "5. **Model Management**: Use the loading capabilities for ongoing model operations\n",
    "\n",
    "The framework maintains all the efficiency and proven patterns from your original notebook while providing the structure and capabilities needed for systematic benchmarking at scale!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
