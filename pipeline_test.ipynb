{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "current_dir = Path.cwd()\n",
    "if str(current_dir) not in sys.path:\n",
    "    sys.path.insert(0, str(current_dir))\n",
    "\n",
    "from src import (\n",
    "    DataConfig, DataLoader,ModelingStrategy, BenchmarkPipeline, create_config\n",
    ")\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_config = DataConfig(\n",
    "    mapping_path = 'data/feature_mapping_train.pkl',\n",
    "    features_path = 'data/processed/train_data_features.feather',\n",
    "    target_path = 'data/train_data_target.feather'\n",
    ")\n",
    "\n",
    "sku_tuples = [(81054, 1334)]\n",
    "pipeline = BenchmarkPipeline(data_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training data shape: (1552, 140)\n",
      "Training target shape: (1552, 2)\n"
     ]
    }
   ],
   "source": [
    "data_loader = DataLoader(data_config)\n",
    "dataset = data_loader.prepare_modeling_dataset(\n",
    "    sku_tuples=[(81054, 1334)],\n",
    "    modeling_strategy=ModelingStrategy.INDIVIDUAL\n",
    ")\n",
    "print(f\"Training data shape: {dataset.X_train.shape}\")\n",
    "print(f\"Training target shape: {dataset.y_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bdID',\n",
       " 'date',\n",
       " 'feature_0000',\n",
       " 'feature_0001',\n",
       " 'feature_0002',\n",
       " 'feature_0003',\n",
       " 'feature_0004',\n",
       " 'feature_0005',\n",
       " 'feature_0006',\n",
       " 'feature_0007',\n",
       " 'feature_0008',\n",
       " 'feature_0009',\n",
       " 'feature_0010',\n",
       " 'feature_0011',\n",
       " 'feature_0012',\n",
       " 'feature_0013',\n",
       " 'feature_0014',\n",
       " 'feature_0015',\n",
       " 'feature_0016',\n",
       " 'feature_0017',\n",
       " 'feature_0018',\n",
       " 'feature_0019',\n",
       " 'feature_0020',\n",
       " 'feature_0021',\n",
       " 'feature_0022',\n",
       " 'feature_0023',\n",
       " 'feature_0024',\n",
       " 'feature_0025',\n",
       " 'feature_0026',\n",
       " 'feature_0027',\n",
       " 'feature_0028',\n",
       " 'feature_0029',\n",
       " 'feature_0030',\n",
       " 'feature_0031',\n",
       " 'feature_0032',\n",
       " 'feature_0033',\n",
       " 'feature_0034',\n",
       " 'feature_0035',\n",
       " 'feature_0036',\n",
       " 'feature_0037',\n",
       " 'feature_0038_lag_1',\n",
       " 'feature_0038_lag_2',\n",
       " 'feature_0038_lag_3',\n",
       " 'feature_0038_lag_4',\n",
       " 'feature_0038_lag_5',\n",
       " 'feature_0038_lag_6',\n",
       " 'feature_0038_lag_7',\n",
       " 'feature_0039']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained 1 XGBoost standard model(s)\n"
     ]
    }
   ],
   "source": [
    "results_xgb_std = pipeline.run_experiment(\n",
    "    sku_tuples=sku_tuples,\n",
    "    modeling_strategy=ModelingStrategy.INDIVIDUAL,\n",
    "    model_type=\"xgboost_standard\",\n",
    "    hyperparameters={\n",
    "        'hidden_size': 64,\n",
    "        'learning_rate': 0.001,  # Fixed parameter name\n",
    "        'num_layers': 2,\n",
    "        'dropout': 0.2,\n",
    "        'max_epochs': 10, \n",
    "        'batch_size': 32,\n",
    "        'deterministic': True,  # For reproducible results\n",
    "        'random_state': 42\n",
    "    },\n",
    "    experiment_name=\"torch_lightning_test\"\n",
    ")\n",
    "\n",
    "print(f\"Trained {results_xgb_std.num_models} Torch Lightning model(s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained 3 XGBoost quantile model(s)\n"
     ]
    }
   ],
   "source": [
    "results_xgb_quantile = pipeline.run_experiment(\n",
    "    sku_tuples=sku_tuples,\n",
    "    modeling_strategy=ModelingStrategy.INDIVIDUAL,\n",
    "    model_type=\"xgboost_quantile\",\n",
    "    hyperparameters={\n",
    "        \"n_estimators\": 100,\n",
    "        \"max_depth\": 6,\n",
    "        \"learning_rate\": 0.3,\n",
    "        \"random_state\": 42\n",
    "    },\n",
    "    quantile_alphas=[0.5, 0.7, 0.9],\n",
    "    experiment_name=\"xgb_quantile_test\"\n",
    ")\n",
    "\n",
    "print(f\"Trained {results_xgb_quantile.num_models} XGBoost quantile model(s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/miniconda3/envs/ml_env/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "\n",
      "  | Name  | Type       | Params | Mode \n",
      "---------------------------------------------\n",
      "0 | model | Sequential | 14.3 K | train\n",
      "---------------------------------------------\n",
      "14.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "14.3 K    Total params\n",
      "0.057     Total estimated model params size (MB)\n",
      "8         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/opt/miniconda3/envs/ml_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results_lightning_std \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43msku_tuples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msku_tuples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodeling_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mModelingStrategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mINDIVIDUAL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlightning_standard\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhyperparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_size\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m132\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhidden_size\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnum_layers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdropout\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_epochs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrandom_state\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m42\u001b[39;49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexperiment_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlightning_standard_test\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     16\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrained \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults_lightning_std\u001b[38;5;241m.\u001b[39mnum_models\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Lightning standard model(s)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/PhD/Transformer Research/Code/Benchmarking/src/pipeline.py:71\u001b[0m, in \u001b[0;36mBenchmarkPipeline.run_experiment\u001b[0;34m(self, sku_tuples, modeling_strategy, model_type, hyperparameters, quantile_alphas, experiment_name, evaluate_on_test, random_state)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m evaluate_on_test:\n\u001b[1;32m     70\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(training_results)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m model(s) on test data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 71\u001b[0m     training_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_results\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# Step 4: Package results\u001b[39;00m\n\u001b[1;32m     74\u001b[0m results \u001b[38;5;241m=\u001b[39m ExperimentResults(\n\u001b[1;32m     75\u001b[0m     training_results\u001b[38;5;241m=\u001b[39mtraining_results,\n\u001b[1;32m     76\u001b[0m     experiment_name\u001b[38;5;241m=\u001b[39mexp_name,\n\u001b[1;32m     77\u001b[0m     config\u001b[38;5;241m=\u001b[39mconfig\n\u001b[1;32m     78\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/PhD/Transformer Research/Code/Benchmarking/src/pipeline.py:189\u001b[0m, in \u001b[0;36mBenchmarkPipeline._evaluate_models\u001b[0;34m(self, training_results, datasets)\u001b[0m\n\u001b[1;32m    186\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_find_dataset_for_result(result, datasets)\n\u001b[1;32m    188\u001b[0m \u001b[38;5;66;03m# Use the simplified evaluator\u001b[39;00m\n\u001b[0;32m--> 189\u001b[0m evaluated_result \u001b[38;5;241m=\u001b[39m \u001b[43mevaluator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_training_result\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my_test\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m evaluated_results\u001b[38;5;241m.\u001b[39mappend(evaluated_result)\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# Log evaluation results\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/PhD/Transformer Research/Code/Benchmarking/src/evaluation.py:69\u001b[0m, in \u001b[0;36mModelEvaluator.evaluate_training_result\u001b[0;34m(self, training_result, X_test, y_test)\u001b[0m\n\u001b[1;32m     62\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics_calc\u001b[38;5;241m.\u001b[39mcalculate_quantile_metrics(\n\u001b[1;32m     63\u001b[0m         y_true\u001b[38;5;241m=\u001b[39my_test_filtered,\n\u001b[1;32m     64\u001b[0m         y_pred\u001b[38;5;241m=\u001b[39my_pred,\n\u001b[1;32m     65\u001b[0m         quantile_alpha\u001b[38;5;241m=\u001b[39mtraining_result\u001b[38;5;241m.\u001b[39mquantile_level\n\u001b[1;32m     66\u001b[0m     )\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# Calculate standard regression metrics\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetrics_calc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalculate_regression_metrics\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_test_filtered\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_pred\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_pred\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Update the TrainingResult with performance metrics\u001b[39;00m\n\u001b[1;32m     75\u001b[0m training_result\u001b[38;5;241m.\u001b[39mperformance_metrics \u001b[38;5;241m=\u001b[39m metrics\n",
      "File \u001b[0;32m~/Documents/PhD/Transformer Research/Code/Benchmarking/src/metrics.py:44\u001b[0m, in \u001b[0;36mMetricsCalculator.calculate_regression_metrics\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput arrays must have the same length\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Core regression metrics\u001b[39;00m\n\u001b[1;32m     43\u001b[0m metrics \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(\u001b[43mmean_squared_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m),\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrmse\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(np\u001b[38;5;241m.\u001b[39msqrt(mean_squared_error(y_true, y_pred))),\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmae\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(mean_absolute_error(y_true, y_pred))\n\u001b[1;32m     47\u001b[0m }\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m metrics\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ml_env/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:218\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    214\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    215\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    216\u001b[0m         )\n\u001b[1;32m    217\u001b[0m     ):\n\u001b[0;32m--> 218\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    224\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    226\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    227\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    228\u001b[0m     )\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ml_env/lib/python3.10/site-packages/sklearn/metrics/_regression.py:580\u001b[0m, in \u001b[0;36mmean_squared_error\u001b[0;34m(y_true, y_pred, sample_weight, multioutput)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Mean squared error regression loss.\u001b[39;00m\n\u001b[1;32m    531\u001b[0m \n\u001b[1;32m    532\u001b[0m \u001b[38;5;124;03mRead more in the :ref:`User Guide <mean_squared_error>`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;124;03m0.825...\u001b[39;00m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    578\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(y_true, y_pred, sample_weight, multioutput)\n\u001b[1;32m    579\u001b[0m _, y_true, y_pred, sample_weight, multioutput \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 580\u001b[0m     \u001b[43m_check_reg_targets_with_floating_dtype\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultioutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    583\u001b[0m )\n\u001b[1;32m    584\u001b[0m output_errors \u001b[38;5;241m=\u001b[39m _average((y_true \u001b[38;5;241m-\u001b[39m y_pred) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, weights\u001b[38;5;241m=\u001b[39msample_weight)\n\u001b[1;32m    586\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(multioutput, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ml_env/lib/python3.10/site-packages/sklearn/metrics/_regression.py:209\u001b[0m, in \u001b[0;36m_check_reg_targets_with_floating_dtype\u001b[0;34m(y_true, y_pred, sample_weight, multioutput, xp)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Ensures y_true, y_pred, and sample_weight correspond to same regression task.\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \n\u001b[1;32m    162\u001b[0m \u001b[38;5;124;03mExtends `_check_reg_targets` by automatically selecting a suitable floating-point\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;124;03m    correct keyword.\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    207\u001b[0m dtype_name \u001b[38;5;241m=\u001b[39m _find_matching_floating_dtype(y_true, y_pred, sample_weight, xp\u001b[38;5;241m=\u001b[39mxp)\n\u001b[0;32m--> 209\u001b[0m y_type, y_true, y_pred, sample_weight, multioutput \u001b[38;5;241m=\u001b[39m \u001b[43m_check_reg_targets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultioutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y_type, y_true, y_pred, sample_weight, multioutput\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ml_env/lib/python3.10/site-packages/sklearn/metrics/_regression.py:116\u001b[0m, in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, sample_weight, multioutput, dtype, xp)\u001b[0m\n\u001b[1;32m    114\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[1;32m    115\u001b[0m y_true \u001b[38;5;241m=\u001b[39m check_array(y_true, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m--> 116\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    118\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m _check_sample_weight(sample_weight, y_true, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ml_env/lib/python3.10/site-packages/sklearn/utils/validation.py:1105\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1099\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1100\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marray\u001b[38;5;241m.\u001b[39mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m while dim <= 2 is required\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1102\u001b[0m     )\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_all_finite:\n\u001b[0;32m-> 1105\u001b[0m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1106\u001b[0m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1107\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1108\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1109\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1110\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[1;32m   1113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[1;32m   1114\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ml_env/lib/python3.10/site-packages/sklearn/utils/validation.py:120\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 120\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ml_env/lib/python3.10/site-packages/sklearn/utils/validation.py:169\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    155\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    168\u001b[0m     )\n\u001b[0;32m--> 169\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN."
     ]
    }
   ],
   "source": [
    "results_lightning_std = pipeline.run_experiment(\n",
    "    sku_tuples=sku_tuples,\n",
    "    modeling_strategy=ModelingStrategy.INDIVIDUAL,\n",
    "    model_type=\"lightning_standard\",\n",
    "    hyperparameters={\n",
    "        \"input_size\": 132,\n",
    "        \"hidden_size\": 128,\n",
    "        \"lr\": 0.001,\n",
    "        \"num_layers\": 2,\n",
    "        \"dropout\": 0.2,\n",
    "        \"max_epochs\": 50,\n",
    "        \"batch_size\": 64,\n",
    "        \"random_state\": 42\n",
    "    },\n",
    "    experiment_name=\"lightning_standard_test\"\n",
    ")\n",
    "\n",
    "print(f\"Trained {results_lightning_std.num_models} Lightning standard model(s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model type: lightning_standard\n",
      "Strategy: individual\n",
      "SKU tuples: [(81054, 1334)]\n",
      "Quantile level: None\n",
      "Training loss: None\n",
      "RMSE: 95.67386493644231\n"
     ]
    }
   ],
   "source": [
    "sample_result = results_lightning_std.training_results[0]\n",
    "print(f\"Model type: {sample_result.model_type}\")\n",
    "print(f\"Strategy: {sample_result.modeling_strategy.value}\")\n",
    "print(f\"SKU tuples: {sample_result.sku_tuples}\")\n",
    "print(f\"Quantile level: {sample_result.quantile_level}\")\n",
    "print(f\"Training loss: {sample_result.training_loss}\")\n",
    "if sample_result.performance_metrics:\n",
    "    print(f\"RMSE: {sample_result.performance_metrics.get('rmse', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type       | Params | Mode \n",
      "---------------------------------------------\n",
      "0 | model | Sequential | 5.1 K  | train\n",
      "---------------------------------------------\n",
      "5.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "5.1 K     Total params\n",
      "0.020     Total estimated model params size (MB)\n",
      "8         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/opt/miniconda3/envs/ml_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:433: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n",
      "/opt/miniconda3/envs/ml_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n",
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type       | Params | Mode \n",
      "---------------------------------------------\n",
      "0 | model | Sequential | 5.1 K  | train\n",
      "---------------------------------------------\n",
      "5.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "5.1 K     Total params\n",
      "0.020     Total estimated model params size (MB)\n",
      "8         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type       | Params | Mode \n",
      "---------------------------------------------\n",
      "0 | model | Sequential | 5.1 K  | train\n",
      "---------------------------------------------\n",
      "5.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "5.1 K     Total params\n",
      "0.020     Total estimated model params size (MB)\n",
      "8         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained 3 Lightning quantile model(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ivn/Documents/PhD/Transformer Research/Code/Benchmarking/src/evaluation.py:57: RuntimeWarning: invalid value encountered in cast\n",
      "  y_pred = np.clip(np.round(y_pred).astype(int), 0, None)\n"
     ]
    }
   ],
   "source": [
    "results_lightning_quantile = pipeline.run_experiment(\n",
    "    sku_tuples=sku_tuples,\n",
    "    modeling_strategy=ModelingStrategy.INDIVIDUAL,\n",
    "    model_type=\"lightning_quantile\",\n",
    "    hyperparameters={\n",
    "        \"hidden_size\": 64,\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"num_layers\": 2,\n",
    "        \"dropout\": 0.2,\n",
    "        \"max_epochs\": 10,\n",
    "        \"batch_size\": 32,\n",
    "        \"random_state\": 42\n",
    "    },\n",
    "    quantile_alphas=[0.7, 0.8, 0.9],\n",
    "    experiment_name=\"lightning_quantile_test\"\n",
    ")\n",
    "\n",
    "print(f\"Trained {results_lightning_quantile.num_models} Lightning quantile model(s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_statquant = pipeline.run_experiment(\n",
    "    sku_tuples=sku_tuples,\n",
    "    modeling_strategy=ModelingStrategy.INDIVIDUAL,\n",
    "    model_type=\"statquant\",\n",
    "    hyperparameters={\n",
    "        \"method\": \"interior-point\",\n",
    "        \"max_iter\": 1000,\n",
    "        \"p_tol\": 1e-6,\n",
    "        \"random_state\": 42\n",
    "    },\n",
    "    quantile_alphas=[0.7],\n",
    "    experiment_name=\"statquant_test\"\n",
    ")\n",
    "\n",
    "print(f\"Trained {results_statquant.num_models} statistical quantile model(s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_sku_tuples = [(81054, 1334), (80558, 1334)]\n",
    "\n",
    "results_combined = pipeline.run_experiment(\n",
    "    sku_tuples=multi_sku_tuples,\n",
    "    modeling_strategy=ModelingStrategy.COMBINED,\n",
    "    model_type=\"xgboost_standard\",\n",
    "    hyperparameters={\n",
    "        \"n_estimators\": 50,\n",
    "        \"max_depth\": 4,\n",
    "        \"learning_rate\": 0.1,\n",
    "        \"random_state\": 42\n",
    "    },\n",
    "    experiment_name=\"combined_strategy_test\"\n",
    ")\n",
    "\n",
    "print(f\"Trained {results_combined.num_models} combined model(s) for {len(multi_sku_tuples)} SKUs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_result = results_xgb_quantile.training_results[0]\n",
    "print(f\"Model type: {sample_result.model_type}\")\n",
    "print(f\"Strategy: {sample_result.modeling_strategy.value}\")\n",
    "print(f\"SKU tuples: {sample_result.sku_tuples}\")\n",
    "print(f\"Quantile level: {sample_result.quantile_level}\")\n",
    "print(f\"Training loss: {sample_result.training_loss}\")\n",
    "if sample_result.performance_metrics:\n",
    "    print(f\"RMSE: {sample_result.performance_metrics.get('rmse', 'N/A')}\")\n",
    "    print(f\"Coverage: {sample_result.performance_metrics.get('coverage_probability', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, result in enumerate(results_lightning_quantile.training_results[:1]):\n",
    "    rmse = result.performance_metrics.get('rmse', 'N/A')\n",
    "    rmse_str = f\"{rmse:.4f}\" if isinstance(rmse, (int, float)) else rmse\n",
    "    print(f\"Model {i+1}: α={result.quantile_level}, RMSE={rmse_str}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
