"""
Model evaluation and benchmarking module.
Provides comprehensive evaluation capabilities across different modeling strategies.
"""

import polars as pl
import pandas as pd
import numpy as np
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from typing import Dict, List, Any, Tuple, Optional
import logging
from pathlib import Path

from .data_structures import BenchmarkModel, ModelingStrategy, ModelRegistry, ModelMetadata
from .data_loading import DataLoader
from .metrics import MetricsCalculator

logger = logging.getLogger(__name__)


class ModelEvaluator:
    """Evaluator for benchmark models with comprehensive metrics and visualizations."""
    
    def __init__(self, data_loader: DataLoader, model_registry: ModelRegistry):
        self.data_loader = data_loader
        self.model_registry = model_registry

    
    def evaluate_trained_model(self, 
                             trained_model: 'TrainedModel',
                             X_test: pl.DataFrame,
                             y_test: pl.DataFrame) -> 'BenchmarkModel':
        """
        Evaluate a TrainedModel and create BenchmarkModel with computed metrics.
        
        This is the core method for post-training evaluation in the pure separation architecture.
        It takes a pure training result (TrainedModel) and computes all performance metrics
        to create the final BenchmarkModel with embedded metrics.
        
        Args:
            trained_model: TrainedModel from pure training (no embedded metrics)
            X_test: Test features DataFrame with bdID column
            y_test: Test targets DataFrame with bdID column
            
        Returns:
            BenchmarkModel with computed performance metrics
        """
        
        logger.info(f"Evaluating trained model: {trained_model.model_type}")
        
        # Use validation bdIDs from trained model's data split
        bdids_to_use = trained_model.data_split.validation_bdIDs
        
        # Filter to validation data
        test_features = X_test.filter(pl.col("bdID").is_in(bdids_to_use))
        test_targets = y_test.filter(pl.col("bdID").is_in(bdids_to_use))
        
        if len(test_features) == 0:
            logger.warning("No test data found for evaluation")
            raise ValueError("No test data available for evaluation")
        
        # Prepare features for prediction using trained model's feature columns
        X_eval = test_features.select(trained_model.feature_columns).to_numpy()
        y_eval = test_targets.select(trained_model.target_column).to_numpy().flatten()
        
        # Make predictions using the trained model
        y_pred = trained_model.model.predict(X_eval)
        y_pred = np.clip(np.round(y_pred).astype(int), 0, None)
        
        # Calculate metrics using centralized metrics calculator
        # Check if this is a quantile model to determine quantile_alpha
        quantile_alpha = trained_model.quantile_level
        metrics = MetricsCalculator.calculate_all_metrics(y_eval, y_pred, quantile_alpha)
        
        # Get basic metadata from trained model
        basic_metadata = trained_model.get_basic_metadata()
        
        # Create ModelMetadata with computed performance metrics
        metadata = ModelMetadata(
            model_id="",  # Will be generated by BenchmarkModel.get_identifier()
            modeling_strategy=basic_metadata['modeling_strategy'],
            sku_tuples=basic_metadata['sku_tuples'],
            model_type=basic_metadata['model_type'],
            store_id=basic_metadata['store_id'],
            product_id=basic_metadata['product_id'],
            hyperparameters=basic_metadata['hyperparameters'],
            training_config=basic_metadata['training_config'],
            performance_metrics=metrics,  # Computed metrics embedded here
            feature_columns=basic_metadata['feature_columns'],
            target_column=basic_metadata['target_column'],
            training_date_range=("", ""),  # TODO: Extract from training config if needed
            model_instance=basic_metadata['model_instance'],
            quantile_level=basic_metadata['quantile_level']
        )
        
        # Create and return BenchmarkModel with embedded metrics
        benchmark_model = BenchmarkModel(
            metadata=metadata,
            model=trained_model.model,
            data_split=trained_model.data_split
        )
        
        logger.info(f"Created BenchmarkModel with metrics: RMSE={metrics.get('rmse', 'N/A'):.4f}")
        if quantile_alpha is not None:
            logger.info(f"Quantile Î±={quantile_alpha}: Coverage={metrics.get('coverage_probability', 'N/A'):.4f}")
        
        return benchmark_model
    
    def evaluate_model(self, 
                      model: BenchmarkModel,
                      test_bdIDs: Optional[np.ndarray] = None) -> Dict[str, Any]:
        """
        Evaluate a single model on test data.
        
        Args:
            model: BenchmarkModel to evaluate
            test_bdIDs: Optional test set bdIDs. If None, uses validation set.
            
        Returns:
            Dictionary with evaluation results
        """
        logger.info(f"Evaluating model: {model.get_identifier()}")
        
        # Determine which bdIDs to use
        if test_bdIDs is None:
            bdids_to_use = model.data_split.validation_bdIDs
            data_split_name = "validation"
        else:
            bdids_to_use = test_bdIDs
            data_split_name = "test"
        
        # Ensure data is loaded in DataLoader
        if not self.data_loader._is_loaded:
            self.data_loader.load_data(lazy=False)
        
        # Get features and target DataFrames from DataLoader
        features_df = self.data_loader._features_df
        target_df = self.data_loader._target_df
        
        # Prepare features for modeling using the correct method
        X, y, feature_cols = self.data_loader._prepare_features(features_df, target_df)
        
        # Filter to test/validation set
        test_features = X.filter(pl.col("bdID").is_in(bdids_to_use))
        test_target = y.filter(pl.col("bdID").is_in(bdids_to_use))
        
        if len(test_features) == 0:
            logger.warning("No test data found for evaluation")
            return {"error": "No test data available"}
        
        # Prepare features for prediction using model's original feature columns
        X_test = test_features.select(model.metadata.feature_columns).to_numpy()
        y_test = test_target.select(model.metadata.target_column).to_numpy().flatten()
        
        # Make predictions
        y_pred = model.model.predict(X_test)
        y_pred = np.clip(np.round(y_pred).astype(int), 0, None)
        
        # Calculate metrics using centralized metrics calculator
        from .metrics import MetricsCalculator
        
        # Check if this is a quantile model to determine quantile_alpha
        quantile_alpha = None
        if hasattr(model.model, 'quantile_alpha'):
            quantile_alpha = model.model.quantile_alpha
        
        metrics = MetricsCalculator.calculate_all_metrics(y_test, y_pred, quantile_alpha)
        
        # Create evaluation result
        evaluation_result = {
            "model_id": model.get_identifier(),
            "modeling_strategy": model.metadata.modeling_strategy.value,
            "sku_tuples": model.metadata.sku_tuples,
            "n_samples": len(y_test),
            "predictions": y_pred.tolist(),
            "actuals": y_test.tolist(),
            "prediction_errors": (y_test - y_pred).tolist(),
            "metrics": metrics,
            "data_split_name": data_split_name
        }
        
        return evaluation_result
    
    def evaluate_model_with_data(self, model: BenchmarkModel, X_data: pl.DataFrame, y_data: pl.DataFrame) -> Dict[str, Any]:
        """
        Evaluate a model using pre-engineered X and y data.
        This is more suitable for notebook usage.
        
        Args:
            model: BenchmarkModel to evaluate
            X_data: Pre-engineered features DataFrame with bdID column
            y_data: Target DataFrame with bdID column
            
        Returns:
            Dictionary with evaluation results
        """
        logger.info(f"Evaluating model with provided data: {model.get_identifier()}")
        
        # Use validation bdIDs from the model's data split
        bdids_to_use = model.data_split.validation_bdIDs
        
        # Filter to validation data
        test_features = X_data.filter(pl.col("bdID").is_in(bdids_to_use))
        test_target = y_data.filter(pl.col("bdID").is_in(bdids_to_use))
        
        if len(test_features) == 0:
            logger.warning("No test data found for evaluation")
            return {"error": "No test data available"}
        
        # Prepare features for prediction
        X_test = test_features.select(model.metadata.feature_columns).to_numpy()
        y_test = test_target.select(model.metadata.target_column).to_numpy().flatten()
        
        # Make predictions using the underlying model object
        underlying_model = model.model
        y_pred = underlying_model.predict(X_test)
        y_pred = np.clip(np.round(y_pred).astype(int), 0, None)
        
        # Check if this is a quantile model to determine quantile_alpha
        quantile_alpha = None
        if hasattr(underlying_model, 'quantile_alpha'):
            quantile_alpha = underlying_model.quantile_alpha
        
        model_metrics = MetricsCalculator.calculate_all_metrics(y_test, y_pred, quantile_alpha)
        
        # Create evaluation result
        evaluation_result = {
            "model_id": model.get_identifier(),
            "modeling_strategy": model.metadata.modeling_strategy.value,
            "sku_tuples": model.metadata.sku_tuples,
            "n_samples": len(y_test),
            "predictions": y_pred.tolist(),
            "actuals": y_test.tolist(),
            "prediction_errors": (y_test - y_pred).tolist(),
            "metrics": model_metrics,
            "data_split_name": "validation"
        }
        
        return evaluation_result
    
    def compare_models(self, 
                      model_ids: List[str],
                      test_bdIDs: Optional[np.ndarray] = None) -> Dict[str, Any]:
        """
        Compare multiple models on the same test data.
        
        Args:
            model_ids: List of model IDs to compare
            test_bdIDs: Optional test set bdIDs
            
        Returns:
            Comparison results dictionary
        """
        logger.info(f"Comparing {len(model_ids)} models")
        
        comparison_results = {
            "model_evaluations": {},
            "metrics_comparison": {},
            "rankings": {}
        }
        
        # Evaluate each model
        all_metrics = {}
        for model_id in model_ids:
            model = self.model_registry.get_model(model_id)
            if model is None:
                logger.warning(f"Model {model_id} not found in registry")
                continue
            
            eval_result = self.evaluate_model(model, test_bdIDs)
            comparison_results["model_evaluations"][model_id] = eval_result
            
            if "metrics" in eval_result:
                all_metrics[model_id] = eval_result["metrics"]
        
        # Compare metrics
        if all_metrics:
            comparison_results["metrics_comparison"] = self._create_metrics_comparison(all_metrics)
            comparison_results["rankings"] = self._rank_models(all_metrics)
        
        return comparison_results
    
    def evaluate_by_modeling_strategy(self, 
                                     modeling_strategy: ModelingStrategy,
                                     test_bdIDs: Optional[np.ndarray] = None) -> Dict[str, Any]:
        """
        Evaluate all models of a specific modeling strategy.
        
        Args:
            modeling_strategy: Modeling strategy to evaluate (COMBINED or INDIVIDUAL)
            test_bdIDs: Optional test set bdIDs
            
        Returns:
            Strategy-level evaluation results
        """
        logger.info(f"Evaluating all models with {modeling_strategy.value} strategy")
        
        # Get all models for this strategy
        model_ids = self.model_registry.list_models(modeling_strategy)
        
        if not model_ids:
            logger.warning(f"No models found for strategy {modeling_strategy.value}")
            return {"error": f"No models found for strategy {modeling_strategy.value}"}
        
        # Compare all models with this strategy
        return self.compare_models(model_ids, test_bdIDs)
    
    def generate_evaluation_report(self, 
                                 evaluation_results: Dict[str, Any],
                                 output_path: Optional[Path] = None) -> str:
        """
        Generate a comprehensive evaluation report.
        
        Args:
            evaluation_results: Results from evaluate_model or compare_models
            output_path: Optional path to save report
            
        Returns:
            Report as string
        """
        report_lines = []
        report_lines.append("# Model Evaluation Report\n")
        
        if "model_evaluations" in evaluation_results:
            # Multiple model comparison report
            report_lines.append("## Model Comparison\n")
            
            # Metrics summary table
            if "metrics_comparison" in evaluation_results:
                report_lines.append("### Performance Metrics\n")
                metrics_df = pd.DataFrame(evaluation_results["metrics_comparison"]).T
                report_lines.append(metrics_df.to_string())
                report_lines.append("\n")
            
            # Rankings
            if "rankings" in evaluation_results:
                report_lines.append("### Model Rankings\n")
                for metric, ranking in evaluation_results["rankings"].items():
                    report_lines.append(f"**{metric.upper()}:**\n")
                    for i, (model_id, value) in enumerate(ranking, 1):
                        report_lines.append(f"{i}. {model_id}: {value:.4f}\n")
                    report_lines.append("\n")
        
        else:
            # Single model report
            model_id = evaluation_results.get("model_id", "Unknown")
            report_lines.append(f"## Model: {model_id}\n")
            
            # Basic info
            report_lines.append(f"**Modeling Strategy:** {evaluation_results.get('modeling_strategy', 'Unknown')}\n")
            report_lines.append(f"**SKU Tuples:** {evaluation_results.get('sku_tuples', [])}\n")
            report_lines.append(f"**Test Samples:** {evaluation_results.get('n_samples', 0)}\n\n")
            
            # Metrics
            if "metrics" in evaluation_results:
                report_lines.append("### Performance Metrics\n")
                for metric, value in evaluation_results["metrics"].items():
                    report_lines.append(f"- **{metric.upper()}:** {value:.4f}\n")
                report_lines.append("\n")
        
        report_text = "".join(report_lines)
        
        # Save to file if requested
        if output_path:
            output_path.parent.mkdir(parents=True, exist_ok=True)
            with open(output_path, 'w') as f:
                f.write(report_text)
            logger.info(f"Report saved to {output_path}")
        
        return report_text