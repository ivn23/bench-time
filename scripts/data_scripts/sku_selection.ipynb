{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1da62ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pickle \n",
    "import typing as t\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a46945",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def create_calendric_features(df: pl.DataFrame, date_column: str) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Create calendric features for a given Polars DataFrame with a date column.\n",
    "\n",
    "    Parameters:\n",
    "        df (pl.DataFrame): Input Polars DataFrame containing a date column.\n",
    "        date_column (str): Name of the column containing dates.\n",
    "\n",
    "    Returns:\n",
    "        pl.DataFrame: Polars DataFrame with added calendric features.\n",
    "    \"\"\"\n",
    "    # Ensure the date column is in datetime format\n",
    "    df = df.with_columns(pl.col(date_column).cast(pl.Date).alias(date_column))\n",
    "\n",
    "    # Create basic calendric features\n",
    "    df = df.with_columns([\n",
    "        pl.col(date_column).dt.month().alias(\"month\"),\n",
    "        (pl.col(date_column).dt.weekday()).alias(\"day_of_week\"),  # Monday=1, ..., Sunday=7\n",
    "        (pl.col(date_column).dt.strftime(\"%V\").cast(pl.Int32)).alias(\"week_of_year\"),  # ISO week number\n",
    "        (pl.col(date_column).dt.year()).alias(\"year\"),\n",
    "        (pl.col(date_column).dt.weekday() >= 5).alias(\"is_weekend\")  # True for Saturday/Sunday\n",
    "    ])\n",
    "\n",
    "    # Create the \"quarter\" column based on the \"month\" column\n",
    "    df = df.with_columns(((pl.col(\"month\") - 1) // 3 + 1).alias(\"quarter\"))\n",
    "\n",
    "    return df\n",
    "\n",
    "def add_lag_features(\n",
    "    df: pl.DataFrame,\n",
    "    lags: t.Union[int, t.List[int], range] = 1,\n",
    "    group_by_cols: t.List[str] = None,\n",
    "    value_col: str = \"value\",\n",
    "    date_col: str = \"date\",\n",
    "    \n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Add lag features to a DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pl.DataFrame\n",
    "        Input DataFrame containing time series data.\n",
    "    value_col : str, default \"value\"\n",
    "        Name of the column to create lag features for.\n",
    "    group_by_cols : List[str], optional\n",
    "        List of columns to group by when creating lags. If None, defaults to \n",
    "        [\"skuID\", \"frequency\"] if both exist, otherwise [\"skuID\"].\n",
    "    date_col : str, default \"date\"\n",
    "        Name of the date column used for sorting.\n",
    "    lags : Union[int, List[int], range], optional\n",
    "        Lag periods to create. If int, creates lags from 1 to that number.\n",
    "        If List[int] or range, creates lags for those specific values.\n",
    "        If None, defaults to range(1, 8).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pl.DataFrame\n",
    "        DataFrame with added lag columns named \"{value_col}_lag_{lag}\".\n",
    "    \"\"\"\n",
    "    \n",
    "    # Handle default parameters\n",
    "    if group_by_cols is None:\n",
    "        group_by_cols = [\"skuID\", \"frequency\"]\n",
    "    \n",
    "    sort_cols = group_by_cols + [date_col]\n",
    "    \n",
    "    # Validate that required columns exist\n",
    "    missing_cols = [col for col in sort_cols + [value_col] if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Missing columns in DataFrame: {missing_cols}\")\n",
    "    \n",
    "    # Sort the DataFrame\n",
    "    df_sorted = df.sort(sort_cols)\n",
    "    \n",
    "    # Create lag features\n",
    "    lag_features = [\n",
    "        pl.col(value_col).shift(lag).over(group_by_cols).alias(f\"{value_col}_lag_{lag}\")\n",
    "        for lag in lags\n",
    "    ]\n",
    "    \n",
    "    # Add lag columns to the DataFrame\n",
    "    result = df_sorted.with_columns(lag_features)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def add_trend_feature(df: pl.DataFrame, date_col: str = \"date\") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds a 'trend' feature to the DataFrame, counting days from the earliest to the latest date.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pl.DataFrame\n",
    "        Input DataFrame containing a date column.\n",
    "    date_col : str, default \"date\"\n",
    "        Name of the date column.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pl.DataFrame\n",
    "        DataFrame with a new 'trend' column joined on the date.\n",
    "    \"\"\"\n",
    "    earliest = df[date_col].min()\n",
    "    latest = df[date_col].max()\n",
    "\n",
    "    # Create a date range from earliest to latest (inclusive)\n",
    "    date_range = pl.date_range(earliest, latest, \"1d\", eager=True)\n",
    "\n",
    "    # Create the trend column: count from 1 to N\n",
    "    trend = pl.int_range(1, len(date_range) + 1, eager=True)\n",
    "\n",
    "    # Create the new DataFrame\n",
    "    result = pl.DataFrame({\n",
    "        date_col: date_range,\n",
    "        \"trend\": trend\n",
    "    })\n",
    "\n",
    "    # Join the trend column to the original DataFrame\n",
    "    df = df.join(result, on=date_col, how=\"left\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737e70aa",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#load csv with skus that are not part of transformers training data\n",
    "unseed_skus_csv =  '../../data/db_snapshot_offsite/unseen_sku/unseen.csv'\n",
    "unseen_skus = pl.read_csv(unseed_skus_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6273ca",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#features_path = '../../data/db_snapshot_offsite/processed/train_data_features.feather'\n",
    "features_path = '../../data/db_snapshot_offsite/train_data/train_data_features.feather'\n",
    "target_path = '../../data/db_snapshot_offsite/train_data/train_data_target.feather'\n",
    "\n",
    "df = pl.read_ipc(features_path)  #Polars uses `read_ipc` for Feather files\n",
    "train_target = pl.read_ipc(target_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d45b1d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "untrained_names = (unseen_skus\n",
    "                   .select(pl.col(\"name\"), pl.col(\"name-2\"), pl.col(\"productID\"), pl.col(\"storeID\"))\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ac7627",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "df_unprocessed = df.join(untrained_names, on=[\"storeID\",\"productID\"], how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6125da",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "df_joined = df_unprocessed.join(train_target.select(\"bdID\",\"target\"), on=\"bdID\", how=\"left\")  # left join to keep only bdIDs that are in features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f820464d",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "df_calendric = create_calendric_features(df_joined, 'date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2770c60b",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "df_dummies = df_calendric.to_dummies(\n",
    "    columns=[\"day_of_week\", \"month\", \"quarter\", \"week_of_year\", \"year\", \"is_weekend\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdac9a5",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "df_lagged = add_lag_features(\n",
    "    df_dummies,\n",
    "    lags=range(1, 8),\n",
    "    group_by_cols=[\"skuID\", \"frequency\"],\n",
    "    value_col=\"target\", # actually target variable\n",
    "    date_col=\"date\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b449e3c8",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "df_trend = add_trend_feature(df_lagged, date_col=\"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bac7dec",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "df_filtered_1 = df_trend.drop_nulls()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f007b87",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "df_filterd_2 = df_trend.filter(pl.col(\"not_for_sale\") != 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed5a43c",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "df_filterd_3 = df_filterd_2.drop(\"lag_target_1\",\"target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1f894b",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "df_clean = df_filterd_3.sort([\"skuID\",\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea1c0a5",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#df_clean.write_ipc(\"../../data/db_snapshot_offsite/train_data/processed/train_data_features.feather\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4611b485",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1912, 7), (377, 1), (715, 7)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sample 3 random productID and storeID combinations simultaniously  and directly pout of the dataframe and give me a list of tuples \n",
    "df_clean.select(pl.col(\"productID\"), pl.col(\"storeID\")).unique().sample(3, seed=RANDOM_STATE).to_dicts()\n",
    "\n",
    "#turn the statement above into a list of tuples\n",
    "sku_tuples = [(d['productID'], d['storeID']) for d in df_clean.select(pl.col(\"productID\"), pl.col(\"storeID\")).unique().sample(3, seed=RANDOM_STATE).to_dicts()]\n",
    "\n",
    "sku_tuples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e9e8329",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_clean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf_clean\u001b[49m\u001b[38;5;241m.\u001b[39mselect(pl\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproductID\u001b[39m\u001b[38;5;124m\"\u001b[39m), pl\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstoreID\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39munique()\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_clean' is not defined"
     ]
    }
   ],
   "source": [
    "df_clean.select(pl.col(\"productID\"), pl.col(\"storeID\")).unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "39b1b7aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2419, 2), (1729, 9), (131, 10)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sku_tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9d7de970",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1, 1)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>target_lag_1</th></tr><tr><td>f64</td></tr></thead><tbody><tr><td>81.0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1, 1)\n",
       "┌──────────────┐\n",
       "│ target_lag_1 │\n",
       "│ ---          │\n",
       "│ f64          │\n",
       "╞══════════════╡\n",
       "│ 81.0         │\n",
       "└──────────────┘"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fitler dataframe for this [(1912, 7)]\n",
    "df_clean.filter((pl.col(\"productID\") == 1912) & (pl.col(\"storeID\") == 7)).select(pl.col(\"target_lag_1\")).sum()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
